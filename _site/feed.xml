<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator><link href="http://localhost:4000/arizeblog/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/arizeblog/" rel="alternate" type="text/html" /><updated>2023-03-14T12:57:05+00:00</updated><id>http://localhost:4000/arizeblog/feed.xml</id><title type="html">ARIZE-BLOG</title><subtitle></subtitle><author><name>Emmanuel Arize</name></author><entry><title type="html">Shape and Append Queries</title><link href="http://localhost:4000/arizeblog/power-query/2023/02/20/Append-Query.html" rel="alternate" type="text/html" title="Shape and Append Queries" /><published>2023-02-20T00:00:00+00:00</published><updated>2023-02-20T00:00:00+00:00</updated><id>http://localhost:4000/arizeblog/power-query/2023/02/20/Append-Query</id><content type="html" xml:base="http://localhost:4000/arizeblog/power-query/2023/02/20/Append-Query.html"><![CDATA[<p>In this tutorial I will be illustrating how to append queries in Power Query. Appending two or more queries in Power Query is one of the most basic and essential tasks that you may need to do in most data preparation scenarios because as a data analyst you may have multiple queries of which you may need to combine to form a single query. Using power query append operation, you can create a single query by adding the contents of one or more queries to another.</p>

<p>When queries are appended the rows from one query are appended (or added) ) at the end of the rows in another query where the column values match.  When a query with 30 rows is appended with another query with 20 rows, this will return a resulting query with 50 rows. In the case of two queries that do not have the same column headers, all column headers from all queries are appended to the resulting query. To append two queries we need data so let now load the data November_2019 and December_2019 below using Power BI.</p>

<p><br />
November_2019
<img class="w3-center" src="/arizeblog/assets/images/power_query/append_001.jpg" /></p>

<p><br /></p>

<p><br />
December_2019
<img class="w3-center" src="/arizeblog/assets/images/power_query/append_01.jpg" /></p>

<p><br /></p>
<h2 id="preprocessing-the-data">Preprocessing the Data</h2>
<p>From the November_2019 data above the first fours rows contain data not relevant to our analysis so we need to remove them.</p>

<p>To remove the first four rows in power query</p>
<ol>
  <li>click on the home tab.</li>
  <li>Under the Reduce Row group click on <strong>Remove Rows</strong> as below.</li>
</ol>

<p><br />
<img class="w3-center" src="/arizeblog/assets/images/power_query/append_02.jpg" /></p>

<p style="margin-left: 50px;">3. From the drop down list select <b>Remove Top Rows</b>.</p>
<p style="margin-left: 50px;">4. In the Remove top rows window, enter 4 in the Number of rows box as below.</p>

<p><br />
<img class="w3-center" src="/arizeblog/assets/images/power_query/append_3.jpg" />
<br /></p>

<p>After clicking ok the operation leaves the headers of the table as the first row shown below.</p>

<p><br />
<img class="w3-center" src="/arizeblog/assets/images/power_query/append_04.jpg" />
<br /></p>

<h3 id="promote-headers-operation">Promote Headers Operation</h3>
<p>The promote headers operation promotes the first row of values as the new column headers.
There are a number of places where we can select the promote headers  operation (<strong>Use First Row as Headers</strong> )</p>

<p>Click on the Home tab then in the Transform group</p>

<p><br />
<img class="w3-center" src="/arizeblog/assets/images/power_query/append_5.jpg" />
<br /></p>

<p>Or On the Transform tab, in the Table group.</p>

<p><br />
<img class="w3-center" src="/arizeblog/assets/images/power_query/append_6.jpg" />
<br /></p>

<p>After clicking on Use First Row as Headers, the first row in the query is used as the column headers of the query and we get the table below</p>

<p><br />
<img class="w3-center" src="/arizeblog/assets/images/power_query/append_7.jpg" />
<br /></p>

<p>For simplicity we will sort the data based on Date column in ascending order and use only the last 5 rows in the table.</p>

<h3 id="sorting--data">Sorting  Data</h3>
<p>to sort the data based on the date column click on the drop down arrow in date column and select <strong>Sort Ascending</strong> as below</p>

<p><br />
 <img class="w3-center" src="/arizeblog/assets/images/power_query/append_8.jpg" />
 <br /></p>

<h3 id="filtering-the-data">Filtering the Data</h3>
<p>To filter the data to contain only the last five rows</p>

<ol>
  <li>click on the home tab.</li>
  <li>Under the Reduce Row group click on <strong>Keep Rows</strong> as below.</li>
  <li>From the drop down list select <strong>Keep Bottom Rows.</strong></li>
  <li>In the Keep Bottom rows window, enter 5 in the Number of rows box as below.</li>
</ol>

<p><br />
<img class="w3-center" src="/arizeblog/assets/images/power_query/append_9.jpg" />
<br /></p>

<p>to get the table below</p>

<p><br />
<img class="w3-center" src="/arizeblog/assets/images/power_query/append_10.jpg" />
<br /></p>

<h3 id="processing-the-second-table-december_2019">Processing the Second table (December_2019)</h3>

<p>In the second table we will apply the same steps used in preprocessing the first table but instead of keeping the last five rows we will keep the top five rows which will give us the table below</p>

<p><br />
<img class="w3-center" src="/arizeblog/assets/images/power_query/append_11.jpg" />
<br /></p>

<h3 id="appending-queries">Appending Queries</h3>
<p>The append operation requires at least two tables. The Append queries command can be found on the Home tab in the Combine group.
<br />
<img class="w3-center" src="/arizeblog/assets/images/power_query/append_12.jpg" />
<br /></p>

<p>On the drop-down menu, you’ll see two options:</p>
<ol>
  <li><strong>Append queries</strong>: which displays the Append dialog box to add additional tables to the
current query.</li>
  <li><strong>Append queries as new</strong> : displays the Append dialog box to create a new query by
appending multiple tables</li>
</ol>

<p>The Append dialog box has two modes:</p>
<ol>
  <li><strong>Two tables</strong>: Combine two table queries together. This mode is the default mode.</li>
  <li><strong>Three or more tables</strong>: Allow an arbitrary number of table queries to be combined.</li>
</ol>

<p>Now let append the November_2019 and December_2019 data as a new query. To append these tables, first select the November_2019 table and on the Home tab, select <strong>Append queries as new</strong>.</p>

<p><br />
<img class="w3-center" src="/arizeblog/assets/images/power_query/append_13.jpg" />
<br /></p>

<p>In the Second table dialog box select the December 2019 data and click ok. After clicking ok a new query named Append1
which contain the total number of rows in December_2019 appended to that of November_2019 as below</p>

<p><br />
<img class="w3-center" src="/arizeblog/assets/images/power_query/append_14.jpg" />
<br /></p>

<p><strong>Note:</strong> In the event that one table doesn’t have columns found in another table, null values will appear in the corresponding column</p>]]></content><author><name>Emmanuel Arize</name></author><category term="Power-Query" /><summary type="html"><![CDATA[In this tutorial I will be illustrating how to append queries in Power Query. Appending two or more queries in Power Query is one of the most basic and essential tasks that you may need to do in most data preparation scenarios because as a data analyst you may have multiple queries of which you may need to combine to form a single query. Using power query append operation, you can create a single query by adding the contents of one or more queries to another.]]></summary></entry><entry><title type="html">Bullet Chart</title><link href="http://localhost:4000/arizeblog/excel/2023/01/05/bullet-graph.html" rel="alternate" type="text/html" title="Bullet Chart" /><published>2023-01-05T00:00:00+00:00</published><updated>2023-01-05T00:00:00+00:00</updated><id>http://localhost:4000/arizeblog/excel/2023/01/05/bullet-graph</id><content type="html" xml:base="http://localhost:4000/arizeblog/excel/2023/01/05/bullet-graph.html"><![CDATA[<p>In the business world, no matter the industry you found yourself, you always have targets and goals that you want to archive. In order to know how you are progressing, you tend to find effective ways to represent your performance against the set target. Using bullet charts you can effectively represent your performance against the target. Bullet charts are variation of bar charts, used for making comparisons such as showing performance metrics against a target.</p>

<h2 id="how-do-you-read-a-bullet-chart">How do you read a bullet chart?</h2>
<p>A bullet chart usually encodes three different data elements:</p>
<ol>
  <li>An observed value known as <strong>Feature or performance Measure</strong> (performance score or the bullet). This is the primary data and is usually encoded as a bar, like the bar on a bar chart (encoded as black bar in the chart below) and centered in the plot area.</li>
  <li>A target value known as <strong>Comparative Measure</strong> used as a target marker to compare against the Feature Measure value.  This is shown in the chart below as a short red line marker that runs perpendicular to the orientation of the chart. Whenever the <strong>Feature Measure</strong> hit or intersect the target, you know you’ve hit your target.</li>
  <li>A qualitative Range of values used for grading. These ranges do  not only communicate the qualitative state of the featured measure but also the degree to which it resides within that state. If for instance the feature measure extends into a range that represents good, the distance that it travels into this range indicates how good it is.</li>
</ol>

<p>For example in the chart below, we are measuring the year to date (YTD) revenue rating for the year. Using this chart, we can see that the YTD rating is 80% and it is shown as a black bar. Using this rating you can easily see that your YTD rating could not hit the target value 90%  which is represented as a red line perpendicular to the black bar. Although your performance came close the target value, you could not archive your set goal or target  which is 90%.</p>

<p><br />
<img src="/arizeblog/assets/images/Excel/bullet1.jpg" />
<br /></p>

<h3 id="steps-to-create-a-bullet-chart-in-excel">Steps to Create a Bullet Chart in Excel</h3>

<p>Using the table below we have the three different data elements namely</p>

<ol>
  <li>First four values are the qualitative range of values used for grading.</li>
  <li>Performance score (feature measure) is the revenue rating for year.</li>
  <li>Target value is the target that we set and wanted to archive for the year.</li>
</ol>

<p>to create the bullet chart.</p>

<p><br />
<img src="/arizeblog/assets/images/Excel/bullet2.jpg" />
<br /></p>

<p>With this data now</p>

<ol>
  <li>Select the needed data (in our case the entire table). After selecting the data click on the <strong>insert tab</strong> and select <strong>recommended Charts</strong>  then <strong>ALL Charts</strong>. From all Charts select Column chart and then a stacked column chart as below</li>
</ol>

<p><br />
<img src="/arizeblog/assets/images/Excel/bullet3.jpg" />
<br /></p>

<p>and click on <strong>ok</strong> to get a chart as below</p>

<p><br />
<img src="/arizeblog/assets/images/Excel/bullet4.jpg" />
<br /></p>
<p>
2. The  range of values on the y-axis is from 0% to 300%, let change it to have values from 0% to 100% by clicking on the
stacked column chart then from <b>Series Options </b> under <b>Format Data Series </b> select <b>vertical (Axis) value</b> .
After selecting vertical (Axis) value, now select <b>Axis Options</b> and click on the <b>Axis Options</b> to change the <b>maximum </b> value from 3.0 in
</p>
<p><br />
<img src="/arizeblog/assets/images/Excel/bullet5.jpg" />
<br /></p>

<p>to 1.0 as below</p>

<p><br />
<img src="/arizeblog/assets/images/Excel/bullet6.jpg" />
<br /></p>
<p>
3. Right-click the stacked column chart and choose Change Series Chart Type. Use the Change Chart
Type dialog box to change the Target series to a Stacked Line with Markers and check the checkbox under the secondary axis and click ok.
</p>

<p><br />
<img src="/arizeblog/assets/images/Excel/bullet7.jpg" />
<br /></p>

<p>After clicking ok to the change, the Target series will show on the chart as a single dot as below.</p>

<p><br />
<img src="/arizeblog/assets/images/Excel/bullet71.jpg" />
<br /></p>

<p>
4. Right-click on the chart and select Format Data Series, then from the <b>Series Options</b> under <b>Format Data Series</b> select <b>series "Target"</b> .

Now under the <b>Fill &amp; Line </b> click on the Marker option and adjust the marker to look like a line and change the size to 20. </p>
<p>Expand the Fill section, select <strong>solid line</strong> and change the color of the solid line to red. Expand the Border section and set the Border to No Line.
<br />
<img src="/arizeblog/assets/images/Excel/bullet8.jpg" />
<br /></p>

<p>After making changes to the maker click on the <strong>Line option</strong> and expand the line section, select <strong>solid line</strong> and change the color of the solid line to red.</p>

<p> 5. Right click on the new secondary axis that was added to the right side of the chart and the delete it to have a chart as below.

<br />
<img src="/arizeblog/assets/images/Excel/bullet9.jpg" />
<br />

<p>
6. Now right-click on the chart and select Format Data Series, then from the <b>Series Options</b> under <b>Format Data Series</b> select <b>series "Performance score"</b>. </p>

<p> 7. In the Format Data Series dialog box, select Secondary Axis. </p>

<p> 8. Still in the Format Data Series dialog box under Series Options, adjust the Gap Width property so that the Performance score series is slightly narrower than the other columns in the chart —here you are using 400% as below.
</p>

<br />
<img src="/arizeblog/assets/images/Excel/bullet10.jpg" />
<br />

<p>
9. Still in the Format Data Series dialog box , under  the <b> Fill &amp; Line option</b>, expand the Fill section, and then select the Solid fill option to set the color of the Performance score series to black as below</p>

<br />
<img src="/arizeblog/assets/images/Excel/bullet11.jpg" />
<br />


<p>10. At this point all that’s left to do is change the color for each qualitative range to incrementally lighter
hues.</p>

<p> 11. After changing the qualitative range to incrementally lighter hues let also change the title to YTD revenue Rating</p>

The final bullet chart is given below

<br />
<img src="/arizeblog/assets/images/Excel/bullet12.jpg" />
<br />
</p>]]></content><author><name>Emmanuel Arize</name></author><category term="Excel" /><summary type="html"><![CDATA[In the business world, no matter the industry you found yourself, you always have targets and goals that you want to archive. In order to know how you are progressing, you tend to find effective ways to represent your performance against the set target. Using bullet charts you can effectively represent your performance against the target. Bullet charts are variation of bar charts, used for making comparisons such as showing performance metrics against a target.]]></summary></entry><entry><title type="html">Basics of DAX TREATAS Function</title><link href="http://localhost:4000/arizeblog/power-bi/2022/10/21/Treatas.html" rel="alternate" type="text/html" title="Basics of DAX TREATAS Function" /><published>2022-10-21T00:00:00+00:00</published><updated>2022-10-21T00:00:00+00:00</updated><id>http://localhost:4000/arizeblog/power-bi/2022/10/21/Treatas</id><content type="html" xml:base="http://localhost:4000/arizeblog/power-bi/2022/10/21/Treatas.html"><![CDATA[<p>In this tutorial, we’ll dive into the DAX TREATAS function in Power BI. I will in the first section of this tutorial show how to create a virtual relationship between two tables  and then move on to show how we can simplify filtering expression using the TREATAS function.</p>

<h3 id="virtual-relationship-using-treatas">Virtual Relationship using TREATAS</h3>
<p>The Best practice to move a filter from one table to another is to create a physical relationship between the two tables
involved. What if the relationship doesn’t exist, is there a way to create virtual relationship between these two tables to
 mimic a physical relationship so that the user thinks the is relationship in place, even if there is none? Using DAX functions 
 I will create virtual relationship between tables that mimic physical relationship. This technique is useful whenever a relationship does not exist. 
 In order to show how we can create a virtual relationship between the Reseller and Sales tables, I have deleted the physical relationship 
 existing between these two tables using data from <a href="https://github.com/microsoft/powerbi-desktop-samples/blob/main/DAX/Adventure%20Works%20DW%202020.pbix">Adventure Works DW 2020<a></a> which can be seen from the data model below</a></p>

<p><br />
<img class="w3-center w3-brown" src="/arizeblog/assets/images/power_bi/treatas1.jpg" />
<br /></p>

<p>Without any physical relationship between the Reseller and Sales tables, let look at the amount of sales by country using <a href="https://daxstudio.org" target="_blank"> DAX Studio</a></p>

<pre><code class="language-{DAX}">EVALUATE
	SUMMARIZECOLUMNS(
              Reseller[Country],
             "Sales Amount",[Sales Amt]
	)
</code></pre>

<p><img class="w3-center w3-brown" src="/arizeblog/assets/images/power_bi/treatas2.jpg" /></p>

<p>From the preceding figure we see that the result for the sum of sales amount is the same for all the countries involved because there no relationship between the two tables.</p>

<p>Using <a href="https://learn.microsoft.com/en-us/dax/treatas-function"> TREATAS</a> which apply the result of a table expression as filters to columns from an unrelated table and define as</p>

<pre><code class="language-{dax}">TREATAS(table_expression, &lt;column&gt;[, &lt;column&gt;[, &lt;column&gt;[,…]]]} )
</code></pre>

<p>we can create a virtual relationship between the two tables to propagate the filter context from Reseller to Sales using the TREATAS function and is given by the code below.</p>

<p><br />
<img class="w3-center w3-brown" src="/arizeblog/assets/images/power_bi/treatas3.jpg" /></p>

<p>Using TREATAS we connected the two tables using the ResellerKey column, and the preceding figure shows that the amount of sales is filtered by Country, even though there is no physical relationship between the two tables.</p>

<p>Let now create a physical relationship between the Reseller and Sales tables using the ResellerKey column in power BI as  below</p>

<p><br />
<img class="w3-center w3-brown" src="/arizeblog/assets/images/power_bi/treatas4.jpg" />
<br /></p>

<p>With a physical relationship between the tables let check if we will get the same result for the amount of sales filtered by country as that of the virtual relationship.</p>

<p>Result of the Data model with physical relationship.<br />
<img class="w3-center w3-brown" src="/arizeblog/assets/images/power_bi/treatas05.jpg" />
<br /></p>

<p>Result of the Data model with virtual relationship.<br />
<img class="w3-center w3-brown" src="/arizeblog/assets/images/power_bi/treatas6.jpg" />
<br /></p>
<h3 id="filtering-values-using-treatas">Filtering Values Using TREATAS</h3>
<p>In this section of the tutorial we will be filtering values using the TREATAS function. Before we start using TREATAS to filter values, we need to create two calculated columns since TREATAS filters using columns and the columns needed for the filtering are not part of data model.</p>

<p>Let add two calculated two columns name <strong>Year</strong> and <strong>Month Num</strong> to the date table which contain the year and the number representing the month.
<br /></p>
<pre><code class="language-{DAX}">Year = YEAR([Date])
</code></pre>
<p><br /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Month Num = MONTH('Date'[Date])
</code></pre></div></div>
<p><br /></p>

<p>With these calculated columns let now create two measures <strong>2018/19 Sales</strong> and <strong>2018/19 Sales_T</strong> which filter the amount of sales for 2018 and 2019. Using the filter function and TREATAS function respectively.</p>

<p><br /></p>

<p><img class="w3-center w3-brown" src="/arizeblog/assets/images/power_bi/treatas7.jpg" />
<br /></p>

<p><img class="w3-center w3-brown" src="/arizeblog/assets/images/power_bi/treatas8.jpg" />
<br /></p>

<p>Let now use the report canvas to display the values for the two measures created
<br /></p>

<p><img class="w3-center w3-brown" src="/arizeblog/assets/images/power_bi/treatas9.jpg" />
<br /></p>

<p>Continuing the filtering examples let create additional two measures <strong>12-2018/1-2019 Sales</strong> and <strong>12-2018/1-2019 Sales_T</strong> which filter the amount of sales for 12/2018 and 1/2019, using the filter function and TREATAS function respectively and where 1 and 12 represent months. For instance 1=January.</p>

<p><br /></p>

<p><img class="w3-center w3-brown" src="/arizeblog/assets/images/power_bi/treatas10.jpg" />
<br /></p>

<p><img class="w3-center w3-brown" src="/arizeblog/assets/images/power_bi/treatas11.jpg" />
<br /></p>

<p>Let now use the report canvas to display the values for <strong>12-2018/1-2019 Sales</strong> and <strong>12-2018/1-2019 Sales_T</strong>
<br /></p>

<p><img class="w3-center w3-brown" src="/arizeblog/assets/images/power_bi/treatas12.jpg" />
<br /></p>]]></content><author><name>Emmanuel Arize</name></author><category term="Power-BI" /><summary type="html"><![CDATA[In this tutorial, we’ll dive into the DAX TREATAS function in Power BI. I will in the first section of this tutorial show how to create a virtual relationship between two tables and then move on to show how we can simplify filtering expression using the TREATAS function.]]></summary></entry><entry><title type="html">Excel IF Function</title><link href="http://localhost:4000/arizeblog/excel/2022/10/20/IF-Statement.html" rel="alternate" type="text/html" title="Excel IF Function" /><published>2022-10-20T00:00:00+00:00</published><updated>2022-10-20T00:00:00+00:00</updated><id>http://localhost:4000/arizeblog/excel/2022/10/20/IF-Statement</id><content type="html" xml:base="http://localhost:4000/arizeblog/excel/2022/10/20/IF-Statement.html"><![CDATA[<p>In this tutorial I will be illustrating how to use EXCEL IF function. The IF function is one of the most used functions in Excel which returns values based on a <strong>true</strong> or <strong>false condition</strong>.The condition here is referred to as logical_test and the function has the following syntax.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>=IF(logical_test, [value_if_true], [value_if_false])
</code></pre></div></div>

<p>For example, =IF(A2=100,”High”,”Low”) says IF(A2= 100, then return High, otherwise return Low).</p>

<p>Let now look at the worksheet below</p>

<p><br />
<img class="w3-center" src="/arizeblog/assets/images/Excel/if1.jpg" />
<br />
Using the preceding the figure above let now rate the sales amount as <strong>high</strong> if sales amount is equal or greater than 100 or low using if less than 100 using the if function.</p>

<p>Let name cell C1 AS <strong>Sales Rating</strong>,</p>

<p><br />
<img class="w3-center" src="/arizeblog/assets/images/Excel/if2.jpg" />
<br /></p>

<ol>
  <li>Select the cell <strong>C2</strong></li>
  <li>Type <strong>=IF</strong></li>
  <li>Double click the IF command</li>
</ol>

<p><br />
 <img class="w3-center" src="/arizeblog/assets/images/Excel/if3.jpg" />
 <br /></p>

<ol>
  <li>Specify the condition B2&gt;=100</li>
  <li>Type ,</li>
  <li>Specify the value “High” for when the condition is TRUE</li>
  <li>Type ,</li>
  <li>Specify the value “Low” for when the condition is FALSE</li>
  <li>Type )</li>
  <li>Hit enter</li>
</ol>

<p><br />
 <img class="w3-center" src="/arizeblog/assets/images/Excel/if4.jpg" />
 <br /></p>

<p>Since the value in cell B2 is greater than 100, the condition is true so the function returns “High”.</p>

<p><br />
<img class="w3-center" src="/arizeblog/assets/images/Excel/if5.jpg" />
<br /></p>

<p>Click on cell C2 and drag it down to populate the other cells as shown below,
<br />
<img class="w3-center" src="/arizeblog/assets/images/Excel/if6.jpg" />
<br /></p>

<h3 id="nested-if-function">Nested IF Function</h3>
<p>Using the IF function it is also possible to use an IF statement as a TRUE or FALSE value inside another IF function and in this way you can test for more than one condition within one function and return more than two results. Let illustrate this with an example.</p>

<p>In this example we are rating the sales amount as <strong>high</strong> if THE amount is equal or greater than 150 and <strong>Medium</strong> if greater than or equal 100 but less than 150 and <strong>low</strong> otherwise using nested if function.</p>

<p>Let name cell D1 AS <strong>Sales Rating 2</strong> and in cell D2 type,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>=IF( B2&gt;=150, "High",
     IF(B2&gt;=100,"Medium","Low")
    )
</code></pre></div></div>
<p><br />
<img class="w3-center" src="/arizeblog/assets/images/Excel/if7.jpg" />
<br /></p>

<p>Click on cell D2 and drag it down to populate the other cells as shown below,
<br />
<img class="w3-center" src="/arizeblog/assets/images/Excel/if8.jpg" />
<br /></p>]]></content><author><name>Emmanuel Arize</name></author><category term="Excel" /><summary type="html"><![CDATA[In this tutorial I will be illustrating how to use EXCEL IF function. The IF function is one of the most used functions in Excel which returns values based on a true or false condition.The condition here is referred to as logical_test and the function has the following syntax.]]></summary></entry><entry><title type="html">DAX FILTER Function</title><link href="http://localhost:4000/arizeblog/power-bi/2022/10/20/Filter.html" rel="alternate" type="text/html" title="DAX FILTER Function" /><published>2022-10-20T00:00:00+00:00</published><updated>2022-10-20T00:00:00+00:00</updated><id>http://localhost:4000/arizeblog/power-bi/2022/10/20/Filter</id><content type="html" xml:base="http://localhost:4000/arizeblog/power-bi/2022/10/20/Filter.html"><![CDATA[<p>This tutorial illustrate how to filter a table using the FILTER function in DAX. Whether you are a data scientist, a marketing or sales person, filtering data is one of the most common ways you analyze data. The FILTER function is often used to filter rows of a table  reducing the size of the table. When table is filtered, the resulting data is a subset of table and this presents an opportunity to prepare for future analysis.</p>

<p>The syntax of FILTER is given as:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>FILTER ( &lt;table&gt;, &lt;condition&gt; )
</code></pre></div></div>
<p>Using this syntax FILTER receives a table and a logical condition as parameters. In order to return a result, it iterates the table evaluating the condition on a row-by-row basis and returns all rows satisfying the condition.</p>

<p>Let now create a measure <strong>Canada Sales</strong> that finds the amount of sales from Canada</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Canada Sales = CALCULATE(
                        Sales[Sales Amt],
                        FILTER(
                                Customer,
                                Customer[Country-Region]="Canada"
                              )
                      )

</code></pre></div></div>
<p>Result from the <strong>Canada Sales</strong> measure shows only sales where Customers are from or in Canada
<br />
<img class="w3-center w3-brown" src="/arizeblog/assets/images/power_bi/filter1.jpg" />
<br /></p>

<p>Let create another measure <strong>Canada USA Sales</strong> that filter the table to return sales where a customer is either from Canada or United States,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Canada USA Sales = CALCULATE(
                            Sales[Sales Amt],
                            FILTER(
                                    Customer,
                                    Customer[Country-Region]="Canada" || Customer[Country-Region]="United States"
                                    )
                            )
</code></pre></div></div>

<p>Result from the <strong>Canada USA Sales</strong> measure shows only sales where Customers are from either Canada OR United States
<br />
<img class="w3-center w3-brown" src="/arizeblog/assets/images/power_bi/filter2.jpg" />
<br /></p>

<h3 id="conclusion">Conclusion</h3>
<p>The filter function is a simple Power BI function that allows you to iterate table on a row-by-row basis and returns all rows satisfying the condition specified.</p>]]></content><author><name>Arize Emmanuel</name></author><category term="Power-BI" /><summary type="html"><![CDATA[This tutorial illustrate how to filter a table using the FILTER function in DAX. Whether you are a data scientist, a marketing or sales person, filtering data is one of the most common ways you analyze data. The FILTER function is often used to filter rows of a table reducing the size of the table. When table is filtered, the resulting data is a subset of table and this presents an opportunity to prepare for future analysis.]]></summary></entry><entry><title type="html">Data Manipulation in R</title><link href="http://localhost:4000/arizeblog/machine-learning-r/2022/05/06/data_manipulation.html" rel="alternate" type="text/html" title="Data Manipulation in R" /><published>2022-05-06T00:00:00+00:00</published><updated>2022-05-06T00:00:00+00:00</updated><id>http://localhost:4000/arizeblog/machine-learning-r/2022/05/06/data_manipulation</id><content type="html" xml:base="http://localhost:4000/arizeblog/machine-learning-r/2022/05/06/data_manipulation.html"><![CDATA[<p>This tutorial introduces how to easily manipulate data in R using the tidyverse package. Data manipulation usually involves</p>

<ul>
  <li>
    <p>computing summary statistics.</p>
  </li>
  <li>
    <p>rows filtering (filter()) and  ordering (arrange()).</p>
  </li>
  <li>
    <p>renaming (rename()) and selecting certain columns (select()</p>
  </li>
</ul>

<p><b>and adding columns</b></p>

<ul>
  <li>
    <p>mutate(): compute and add new variables into a data table. It preserves existing variables and adds new columns at the end of your dataset and</p>
  </li>
  <li>
    <p>transmute(): compute new columns and only keep the new columns,</p>
  </li>
</ul>

<p>setwd(“wdirectory”) Changes the current working directory to wdirectory.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>setwd('C:/Users/USER/Desktop/JUPYTER_NOTEBOOK/A_MYTUTORIALS/MYR')
</code></pre></div></div>

<p>Load the needed packages</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>library(devtools)
library(tidyverse)
library(nycflights13)
library(readxl)
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Loading required package: usethis

Warning message in (function (kind = NULL, normal.kind = NULL, sample.kind = NULL) :
"non-uniform 'Rounding' sampler used"
-- [1mAttaching packages[22m ------------------------------------------------------------------------------- tidyverse 1.3.1 --

[32mv[39m [34mggplot2[39m 3.3.3     [32mv[39m [34mpurrr  [39m 0.3.4
[32mv[39m [34mtibble [39m 3.1.1     [32mv[39m [34mdplyr  [39m 1.0.5
[32mv[39m [34mtidyr  [39m 1.1.3     [32mv[39m [34mstringr[39m 1.4.0
[32mv[39m [34mreadr  [39m 1.4.0     [32mv[39m [34mforcats[39m 0.5.1

Warning message in (function (kind = NULL, normal.kind = NULL, sample.kind = NULL) :
"non-uniform 'Rounding' sampler used"
-- [1mConflicts[22m ---------------------------------------------------------------------------------- tidyverse_conflicts() --
[31mx[39m [34mdplyr[39m::[32mfilter()[39m masks [34mstats[39m::filter()
[31mx[39m [34mdplyr[39m::[32mlag()[39m    masks [34mstats[39m::lag()
</code></pre></div></div>

<h1 id="datatset">Datatset</h1>
<p>We will use titanic dataset. This dataset has 1309 observations with 14 variables. To explore the basic data manipulation verbs of dplyr,
 we start by converting the data into a tibble data frame for easier data manipulation</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>my_data&lt;-as_tibble(read_xls('data/titanic.xls'))
names(my_data)
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Warning message in read_fun(path = enc2native(normalizePath(path)), sheet_i = sheet, :
"Coercing text to numeric in M1306 / R1306C13: '328'"
</code></pre></div></div>

<h1 id="variable-selection">Variable Selection</h1>

<p>we can select or subset variables by names or position.</p>

<p>Under variable selection we will learn how to use</p>

<ul>
  <li>
    <p>select(): allow us to extract variables or variables as a data table and can  also be used to remove variables from the data frame.</p>
  </li>
  <li>
    <p>select_if(): Select variabless based on a particular condition.</p>
  </li>
  <li>
    <p>Variabl Selection by position</p>
  </li>
</ul>

<p>select from the my_data variable positioned from 1 to 4 inclusive</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>head(my_data %&gt;% select(1:4))
</code></pre></div></div>

<table class="dataframe">
<caption>A tibble: 6 × 4</caption>
<thead>
	<tr><th scope="col">pclass</th><th scope="col">survived</th><th scope="col">name</th><th scope="col">sex</th></tr>
	<tr><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;chr&gt;</th></tr>
</thead>
<tbody>
	<tr><td>1</td><td>1</td><td>Allen, Miss. Elisabeth Walton                  </td><td>female</td></tr>
	<tr><td>1</td><td>1</td><td>Allison, Master. Hudson Trevor                 </td><td>male  </td></tr>
	<tr><td>1</td><td>0</td><td>Allison, Miss. Helen Loraine                   </td><td>female</td></tr>
	<tr><td>1</td><td>0</td><td>Allison, Mr. Hudson Joshua Creighton           </td><td>male  </td></tr>
	<tr><td>1</td><td>0</td><td>Allison, Mrs. Hudson J C (Bessie Waldo Daniels)</td><td>female</td></tr>
	<tr><td>1</td><td>1</td><td>Anderson, Mr. Harry                            </td><td>male  </td></tr>
</tbody>
</table>

<p>select variables positioned at 1, 4,6,7</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
head(my_data %&gt;% select(1,4,6,7))

</code></pre></div></div>

<table class="dataframe">
<caption>A tibble: 6 × 4</caption>
<thead>
	<tr><th scope="col">pclass</th><th scope="col">sex</th><th scope="col">sibsp</th><th scope="col">parch</th></tr>
	<tr><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>1</td><td>female</td><td>0</td><td>0</td></tr>
	<tr><td>1</td><td>male  </td><td>1</td><td>2</td></tr>
	<tr><td>1</td><td>female</td><td>1</td><td>2</td></tr>
	<tr><td>1</td><td>male  </td><td>1</td><td>2</td></tr>
	<tr><td>1</td><td>female</td><td>1</td><td>2</td></tr>
	<tr><td>1</td><td>male  </td><td>0</td><td>0</td></tr>
</tbody>
</table>

<h1 id="select-varaibles-by-name">Select varaibles by name</h1>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>head(select(my_data,pclass,name:age),3)
</code></pre></div></div>

<table class="dataframe">
<caption>A tibble: 3 × 4</caption>
<thead>
	<tr><th scope="col">pclass</th><th scope="col">name</th><th scope="col">sex</th><th scope="col">age</th></tr>
	<tr><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>1</td><td>Allen, Miss. Elisabeth Walton </td><td>female</td><td>29.0000</td></tr>
	<tr><td>1</td><td>Allison, Master. Hudson Trevor</td><td>male  </td><td> 0.9167</td></tr>
	<tr><td>1</td><td>Allison, Miss. Helen Loraine  </td><td>female</td><td> 2.0000</td></tr>
</tbody>
</table>

<p>Select all variables except variables from survived to cabin</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>head(select(my_data,-(survived:cabin)),3)
</code></pre></div></div>

<table class="dataframe">
<caption>A tibble: 3 × 5</caption>
<thead>
	<tr><th scope="col">pclass</th><th scope="col">embarked</th><th scope="col">boat</th><th scope="col">body</th><th scope="col">home.dest</th></tr>
	<tr><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;chr&gt;</th></tr>
</thead>
<tbody>
	<tr><td>1</td><td>S</td><td>2 </td><td>NA</td><td>St Louis, MO                   </td></tr>
	<tr><td>1</td><td>S</td><td>11</td><td>NA</td><td>Montreal, PQ / Chesterville, ON</td></tr>
	<tr><td>1</td><td>S</td><td>NA</td><td>NA</td><td>Montreal, PQ / Chesterville, ON</td></tr>
</tbody>
</table>

<p>select variables whose name starts with bo</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>head(my_data %&gt;% select(starts_with('bo')),3)
</code></pre></div></div>

<table class="dataframe">
<caption>A tibble: 3 × 2</caption>
<thead>
	<tr><th scope="col">boat</th><th scope="col">body</th></tr>
	<tr><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>2 </td><td>NA</td></tr>
	<tr><td>11</td><td>NA</td></tr>
	<tr><td>NA</td><td>NA</td></tr>
</tbody>
</table>

<p>select variables whose name ends with t</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>head(my_data %&gt;% select(ends_with('t')),3)
</code></pre></div></div>

<table class="dataframe">
<caption>A tibble: 3 × 3</caption>
<thead>
	<tr><th scope="col">ticket</th><th scope="col">boat</th><th scope="col">home.dest</th></tr>
	<tr><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;chr&gt;</th></tr>
</thead>
<tbody>
	<tr><td>24160 </td><td>2 </td><td>St Louis, MO                   </td></tr>
	<tr><td>113781</td><td>11</td><td>Montreal, PQ / Chesterville, ON</td></tr>
	<tr><td>113781</td><td>NA</td><td>Montreal, PQ / Chesterville, ON</td></tr>
</tbody>
</table>

<p>Select variables whose names contains “me”</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>head(my_data %&gt;% select(contains('me')),4)
</code></pre></div></div>

<table class="dataframe">
<caption>A tibble: 4 × 2</caption>
<thead>
	<tr><th scope="col">name</th><th scope="col">home.dest</th></tr>
	<tr><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;chr&gt;</th></tr>
</thead>
<tbody>
	<tr><td>Allen, Miss. Elisabeth Walton       </td><td>St Louis, MO                   </td></tr>
	<tr><td>Allison, Master. Hudson Trevor      </td><td>Montreal, PQ / Chesterville, ON</td></tr>
	<tr><td>Allison, Miss. Helen Loraine        </td><td>Montreal, PQ / Chesterville, ON</td></tr>
	<tr><td>Allison, Mr. Hudson Joshua Creighton</td><td>Montreal, PQ / Chesterville, ON</td></tr>
</tbody>
</table>

<h1 id="variable-selection-based-on-a-condtion">Variable selection based on a condtion</h1>

<p>select only character variables</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>head(my_data %&gt;% select_if(is.character),4)
</code></pre></div></div>

<table class="dataframe">
<caption>A tibble: 4 × 7</caption>
<thead>
	<tr><th scope="col">name</th><th scope="col">sex</th><th scope="col">ticket</th><th scope="col">cabin</th><th scope="col">embarked</th><th scope="col">boat</th><th scope="col">home.dest</th></tr>
	<tr><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;chr&gt;</th></tr>
</thead>
<tbody>
	<tr><td>Allen, Miss. Elisabeth Walton       </td><td>female</td><td>24160 </td><td>B5     </td><td>S</td><td>2 </td><td>St Louis, MO                   </td></tr>
	<tr><td>Allison, Master. Hudson Trevor      </td><td>male  </td><td>113781</td><td>C22 C26</td><td>S</td><td>11</td><td>Montreal, PQ / Chesterville, ON</td></tr>
	<tr><td>Allison, Miss. Helen Loraine        </td><td>female</td><td>113781</td><td>C22 C26</td><td>S</td><td>NA</td><td>Montreal, PQ / Chesterville, ON</td></tr>
	<tr><td>Allison, Mr. Hudson Joshua Creighton</td><td>male  </td><td>113781</td><td>C22 C26</td><td>S</td><td>NA</td><td>Montreal, PQ / Chesterville, ON</td></tr>
</tbody>
</table>

<p>selecting only numerical variables</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>head(select_if(my_data,is.numeric),4)
</code></pre></div></div>

<table class="dataframe">
<caption>A tibble: 4 × 7</caption>
<thead>
	<tr><th scope="col">pclass</th><th scope="col">survived</th><th scope="col">age</th><th scope="col">sibsp</th><th scope="col">parch</th><th scope="col">fare</th><th scope="col">body</th></tr>
	<tr><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>1</td><td>1</td><td>29.0000</td><td>0</td><td>0</td><td>211.3375</td><td> NA</td></tr>
	<tr><td>1</td><td>1</td><td> 0.9167</td><td>1</td><td>2</td><td>151.5500</td><td> NA</td></tr>
	<tr><td>1</td><td>0</td><td> 2.0000</td><td>1</td><td>2</td><td>151.5500</td><td> NA</td></tr>
	<tr><td>1</td><td>0</td><td>30.0000</td><td>1</td><td>2</td><td>151.5500</td><td>135</td></tr>
</tbody>
</table>

<h1 id="removing-columns">Removing columns</h1>

<p>For simplicity we will work with only few variables</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sub_data&lt;-my_data %&gt;% select(sex,pclass,age,'survived',cabin,'name','sex','age','sibsp')
</code></pre></div></div>

<p>remove variables named pclass,age and sex</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>head(sub_data%&gt;% select(-sex,-pclass,-age),3)
</code></pre></div></div>

<table class="dataframe">
<caption>A tibble: 3 × 4</caption>
<thead>
	<tr><th scope="col">survived</th><th scope="col">cabin</th><th scope="col">name</th><th scope="col">sibsp</th></tr>
	<tr><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>1</td><td>B5     </td><td>Allen, Miss. Elisabeth Walton </td><td>0</td></tr>
	<tr><td>1</td><td>C22 C26</td><td>Allison, Master. Hudson Trevor</td><td>1</td></tr>
	<tr><td>0</td><td>C22 C26</td><td>Allison, Miss. Helen Loraine  </td><td>1</td></tr>
</tbody>
</table>

<h1 id="rows-filtering-filter">Rows filtering (filter())</h1>

<p>This section describes how to subset or extract samples or rows from the dataset based on certain criteria</p>

<p>extract male (sex==’male’) passengers who survived (survived==1)  and has sibsp==1 (Number of Siblings/Spouses Aboard)</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>head(sub_data %&gt;% filter(sex=='male' &amp; sibsp==1 &amp; survived==1),2)
</code></pre></div></div>

<table class="dataframe">
<caption>A tibble: 2 × 7</caption>
<thead>
	<tr><th scope="col">sex</th><th scope="col">pclass</th><th scope="col">age</th><th scope="col">survived</th><th scope="col">cabin</th><th scope="col">name</th><th scope="col">sibsp</th></tr>
	<tr><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>male</td><td>1</td><td> 0.9167</td><td>1</td><td>C22 C26</td><td>Allison, Master. Hudson Trevor</td><td>1</td></tr>
	<tr><td>male</td><td>1</td><td>37.0000</td><td>1</td><td>D35    </td><td>Beckwith, Mr. Richard Leonard </td><td>1</td></tr>
</tbody>
</table>

<p>OR</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>head(sub_data %&gt;% filter(sex=='male', sibsp==1,survived==1),2)
</code></pre></div></div>

<table class="dataframe">
<caption>A tibble: 2 × 7</caption>
<thead>
	<tr><th scope="col">sex</th><th scope="col">pclass</th><th scope="col">age</th><th scope="col">survived</th><th scope="col">cabin</th><th scope="col">name</th><th scope="col">sibsp</th></tr>
	<tr><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>male</td><td>1</td><td> 0.9167</td><td>1</td><td>C22 C26</td><td>Allison, Master. Hudson Trevor</td><td>1</td></tr>
	<tr><td>male</td><td>1</td><td>37.0000</td><td>1</td><td>D35    </td><td>Beckwith, Mr. Richard Leonard </td><td>1</td></tr>
</tbody>
</table>

<p>extract rows where  passengers are male(sex==’male’) or survived (survived==1)  or has sibsp==1 or 2 (Number of Siblings/Spouses Aboard)</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>head(sub_data %&gt;% filter(sex=='male' | sibsp==1 |sibsp==2| survived==1),3)
</code></pre></div></div>

<table class="dataframe">
<caption>A tibble: 3 × 7</caption>
<thead>
	<tr><th scope="col">sex</th><th scope="col">pclass</th><th scope="col">age</th><th scope="col">survived</th><th scope="col">cabin</th><th scope="col">name</th><th scope="col">sibsp</th></tr>
	<tr><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>female</td><td>1</td><td>29.0000</td><td>1</td><td>B5     </td><td>Allen, Miss. Elisabeth Walton </td><td>0</td></tr>
	<tr><td>male  </td><td>1</td><td> 0.9167</td><td>1</td><td>C22 C26</td><td>Allison, Master. Hudson Trevor</td><td>1</td></tr>
	<tr><td>female</td><td>1</td><td> 2.0000</td><td>0</td><td>C22 C26</td><td>Allison, Miss. Helen Loraine  </td><td>1</td></tr>
</tbody>
</table>

<p>select variables sibsp,sex,age and from these variables extract rows where age&lt;10</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>head(sub_data %&gt;% select(sex,sibsp,age) %&gt;%filter(age&lt;10),3)

</code></pre></div></div>

<table class="dataframe">
<caption>A tibble: 3 × 3</caption>
<thead>
	<tr><th scope="col">sex</th><th scope="col">sibsp</th><th scope="col">age</th></tr>
	<tr><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>male  </td><td>1</td><td>0.9167</td></tr>
	<tr><td>female</td><td>1</td><td>2.0000</td></tr>
	<tr><td>male  </td><td>0</td><td>4.0000</td></tr>
</tbody>
</table>

<h1 id="selecting-random-rows-or-samples-from-a-dataset">Selecting random rows or samples from a dataset</h1>

<p>selecting 10 random samples without replacement from the data</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>head(sub_data %&gt;% sample_n(10,replace = FALSE),2)
</code></pre></div></div>

<table class="dataframe">
<caption>A tibble: 2 × 7</caption>
<thead>
	<tr><th scope="col">sex</th><th scope="col">pclass</th><th scope="col">age</th><th scope="col">survived</th><th scope="col">cabin</th><th scope="col">name</th><th scope="col">sibsp</th></tr>
	<tr><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>male  </td><td>3</td><td>20</td><td>0</td><td>NA  </td><td>Vendel, Mr. Olof Edvin                      </td><td>0</td></tr>
	<tr><td>female</td><td>1</td><td>35</td><td>1</td><td>C123</td><td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td><td>1</td></tr>
</tbody>
</table>

<p>Select 1% random samples without replacement from the data</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sub_data %&gt;% sample_frac(0.01,replace = FALSE)
</code></pre></div></div>

<table class="dataframe">
<caption>A tibble: 13 × 7</caption>
<thead>
	<tr><th scope="col">sex</th><th scope="col">pclass</th><th scope="col">age</th><th scope="col">survived</th><th scope="col">cabin</th><th scope="col">name</th><th scope="col">sibsp</th></tr>
	<tr><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>male  </td><td>1</td><td>34</td><td>1</td><td>NA </td><td>Seward, Mr. Frederic Kimber                 </td><td>0</td></tr>
	<tr><td>male  </td><td>3</td><td>44</td><td>0</td><td>NA </td><td>Cribb, Mr. John Hatfield                    </td><td>0</td></tr>
	<tr><td>male  </td><td>3</td><td>23</td><td>1</td><td>NA </td><td>Asplund, Mr. Johan Charles                  </td><td>0</td></tr>
	<tr><td>male  </td><td>3</td><td>26</td><td>0</td><td>NA </td><td>Bostandyeff, Mr. Guentcho                   </td><td>0</td></tr>
	<tr><td>male  </td><td>2</td><td>27</td><td>0</td><td>NA </td><td>Pulbaum, Mr. Franz                          </td><td>0</td></tr>
	<tr><td>male  </td><td>3</td><td>17</td><td>0</td><td>NA </td><td>Elias, Mr. Joseph Jr                        </td><td>1</td></tr>
	<tr><td>male  </td><td>1</td><td>41</td><td>0</td><td>D21</td><td>Kenyon, Mr. Frederick R                     </td><td>1</td></tr>
	<tr><td>male  </td><td>3</td><td>31</td><td>1</td><td>NA </td><td>Stranden, Mr. Juho                          </td><td>0</td></tr>
	<tr><td>female</td><td>2</td><td>25</td><td>1</td><td>NA </td><td>Shelley, Mrs. William (Imanita Parrish Hall)</td><td>0</td></tr>
	<tr><td>male  </td><td>3</td><td>NA</td><td>0</td><td>NA </td><td>Petroff, Mr. Pastcho ("Pentcho")            </td><td>0</td></tr>
	<tr><td>male  </td><td>3</td><td>NA</td><td>1</td><td>NA </td><td>O'Keefe, Mr. Patrick                        </td><td>0</td></tr>
	<tr><td>male  </td><td>2</td><td> 2</td><td>1</td><td>NA </td><td>Wells, Master. Ralph Lester                 </td><td>1</td></tr>
	<tr><td>male  </td><td>3</td><td>13</td><td>0</td><td>NA </td><td>Asplund, Master. Filip Oscar                </td><td>4</td></tr>
</tbody>
</table>

<h1 id="missing-values">Missing values</h1>

<p>Number of missing values in the age variable</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sub_data %&gt;% summarise(num_na=sum(is.na(age)))
</code></pre></div></div>

<table class="dataframe">
<caption>A tibble: 1 × 1</caption>
<thead>
	<tr><th scope="col">num_na</th></tr>
	<tr><th scope="col">&lt;int&gt;</th></tr>
</thead>
<tbody>
	<tr><td>263</td></tr>
</tbody>
</table>

<p>number of missing values in each variable</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sub_data %&gt;% purrr::map_df(~sum(is.na(.)))
</code></pre></div></div>

<table class="dataframe">
<caption>A tibble: 1 × 7</caption>
<thead>
	<tr><th scope="col">sex</th><th scope="col">pclass</th><th scope="col">age</th><th scope="col">survived</th><th scope="col">cabin</th><th scope="col">name</th><th scope="col">sibsp</th></tr>
	<tr><th scope="col">&lt;int&gt;</th><th scope="col">&lt;int&gt;</th><th scope="col">&lt;int&gt;</th><th scope="col">&lt;int&gt;</th><th scope="col">&lt;int&gt;</th><th scope="col">&lt;int&gt;</th><th scope="col">&lt;int&gt;</th></tr>
</thead>
<tbody>
	<tr><td>0</td><td>0</td><td>263</td><td>0</td><td>1014</td><td>0</td><td>0</td></tr>
</tbody>
</table>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
nrow(sub_data)
</code></pre></div></div>

<p>1309</p>

<p>drop samples of the variables age and cabin with nas</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
no_nas&lt;-sub_data %&gt;% filter_at(vars(age,cabin),all_vars(!is.na(.)))
nrow(no_nas)
</code></pre></div></div>

<p>272</p>

<h1 id="adding-new-variables">Adding New Variables</h1>

<p>Under this section we will use the housing dataset</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>housing&lt;-readr::read_csv('data/housing.csv',guess_max = 20)
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[36m--[39m [1m[1mColumn specification[1m[22m [36m------------------------------------------------------------------------------------------------[39m
cols(
  longitude = [32mcol_double()[39m,
  latitude = [32mcol_double()[39m,
  housing_median_age = [32mcol_double()[39m,
  total_rooms = [32mcol_double()[39m,
  total_bedrooms = [32mcol_double()[39m,
  population = [32mcol_double()[39m,
  households = [32mcol_double()[39m,
  median_income = [32mcol_double()[39m,
  median_house_value = [32mcol_double()[39m,
  ocean_proximity = [31mcol_character()[39m
)
</code></pre></div></div>

<p>Under this section we will select only few variables needed to created new variables</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>housing&lt;-housing %&gt;% select(total_rooms,households,total_bedrooms,total_rooms,
        population,households,ocean_proximity,median_income)
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<h1 id="mutate">mutate()</h1>
<p>mutate() adds new variables at the end of your dataset</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>head(housing 
      
      %&gt;% mutate( rooms_per_household= total_rooms/households,
                  bedrooms_per_room=total_bedrooms/total_rooms,
                  population_per_household=population/households
                  ) 
                  %&gt;% select(-c(total_rooms,households,population,total_bedrooms,median_income))
                  ,3)
</code></pre></div></div>

<table class="dataframe">
<caption>A tibble: 3 × 4</caption>
<thead>
	<tr><th scope="col">ocean_proximity</th><th scope="col">rooms_per_household</th><th scope="col">bedrooms_per_room</th><th scope="col">population_per_household</th></tr>
	<tr><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>NEAR BAY</td><td>6.984127</td><td>0.1465909</td><td>2.555556</td></tr>
	<tr><td>NEAR BAY</td><td>6.238137</td><td>0.1557966</td><td>2.109842</td></tr>
	<tr><td>NEAR BAY</td><td>8.288136</td><td>0.1295160</td><td>2.802260</td></tr>
</tbody>
</table>

<p>transmute() only keep the new variables created</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>head(housing %&gt;% 
            transmute(
                  rooms_per_household= total_rooms/households,
                  bedrooms_per_room=total_bedrooms/total_rooms,
                  population_per_household=population/households
                   ),
                   3)
</code></pre></div></div>

<table class="dataframe">
<caption>A tibble: 3 × 3</caption>
<thead>
	<tr><th scope="col">rooms_per_household</th><th scope="col">bedrooms_per_room</th><th scope="col">population_per_household</th></tr>
	<tr><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>6.984127</td><td>0.1465909</td><td>2.555556</td></tr>
	<tr><td>6.238137</td><td>0.1557966</td><td>2.109842</td></tr>
	<tr><td>8.288136</td><td>0.1295160</td><td>2.802260</td></tr>
</tbody>
</table>

<h1 id="summary-statistics">Summary Statistics</h1>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>getmode &lt;- function(n) {
   uniqn &lt;- unique(n)
   uniqn[which.max(tabulate(match(n, uniqn)))]
}
housing %&gt;% summarise(count=n(),mean_income=mean(median_income,na.rm=TRUE),
                      mode_income=getmode(median_income))
</code></pre></div></div>

<table class="dataframe">
<caption>A tibble: 1 × 3</caption>
<thead>
	<tr><th scope="col">count</th><th scope="col">mean_income</th><th scope="col">mode_income</th></tr>
	<tr><th scope="col">&lt;int&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>20640</td><td>3.870671</td><td>3.125</td></tr>
</tbody>
</table>

<p>Group by one variable</p>

<ul>
  <li>Note : you can groupe by multiple variables</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>housing %&gt;%group_by(ocean_proximity) %&gt;% summarise(mean_income=mean(median_income,na.rm=TRUE),
                                                   mean_housholde=mean(households))
</code></pre></div></div>

<table class="dataframe">
<caption>A tibble: 5 × 3</caption>
<thead>
	<tr><th scope="col">ocean_proximity</th><th scope="col">mean_income</th><th scope="col">mean_housholde</th></tr>
	<tr><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>&lt;1H OCEAN </td><td>4.230682</td><td>517.7450</td></tr>
	<tr><td>INLAND    </td><td>3.208996</td><td>477.4476</td></tr>
	<tr><td>ISLAND    </td><td>2.744420</td><td>276.6000</td></tr>
	<tr><td>NEAR BAY  </td><td>4.172885</td><td>488.6162</td></tr>
	<tr><td>NEAR OCEAN</td><td>4.005785</td><td>501.2445</td></tr>
</tbody>
</table>

<p>summary statistics on numerical variables group by ocean proximity</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>housing%&gt;% select(-population)%&gt;%group_by(ocean_proximity) %&gt;%
summarise_if(is.numeric, mean, na.rm = TRUE)
</code></pre></div></div>

<table class="dataframe">
<caption>A tibble: 5 × 5</caption>
<thead>
	<tr><th scope="col">ocean_proximity</th><th scope="col">total_rooms</th><th scope="col">households</th><th scope="col">total_bedrooms</th><th scope="col">median_income</th></tr>
	<tr><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>&lt;1H OCEAN </td><td>2628.344</td><td>517.7450</td><td>546.5392</td><td>4.230682</td></tr>
	<tr><td>INLAND    </td><td>2717.743</td><td>477.4476</td><td>533.8816</td><td>3.208996</td></tr>
	<tr><td>ISLAND    </td><td>1574.600</td><td>276.6000</td><td>420.4000</td><td>2.744420</td></tr>
	<tr><td>NEAR BAY  </td><td>2493.590</td><td>488.6162</td><td>514.1828</td><td>4.172885</td></tr>
	<tr><td>NEAR OCEAN</td><td>2583.701</td><td>501.2445</td><td>538.6157</td><td>4.005785</td></tr>
</tbody>
</table>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>]]></content><author><name>Emmanuel Arize</name></author><category term="machine-learning-R" /><summary type="html"><![CDATA[This tutorial introduces how to easily manipulate data in R using the tidyverse package. Data manipulation usually involves]]></summary></entry><entry><title type="html">Introduction to LSTMs</title><link href="http://localhost:4000/arizeblog/machine-learning-python/2021/05/07/INTRODUCTION-TO-LSTM.html" rel="alternate" type="text/html" title="Introduction to LSTMs" /><published>2021-05-07T00:00:00+00:00</published><updated>2021-05-07T00:00:00+00:00</updated><id>http://localhost:4000/arizeblog/machine-learning-python/2021/05/07/INTRODUCTION-TO-LSTM</id><content type="html" xml:base="http://localhost:4000/arizeblog/machine-learning-python/2021/05/07/INTRODUCTION-TO-LSTM.html"><![CDATA[<hr />
<p>In this post, I will explain the internal mechanisms that allow LSTM networks to perform better when  model temporal sequences and their long-range dependencies than the coventional RNNs, We will then use it in real life problem by training LSTM as a multi-class classifier to predict the tag of a programming question on Stack Overflow using Tensorflow/Keras.</p>

<blockquote>
  <p>Note: In order to understand this post, you must have basic knowledge of recurrent neural networks and Keras. You can refer to <a href="https://emmanuel-arize.github.io/datascience-blog/deeplearning/deep-learning/2021/05/06/RNN.html" target="_blank">  recurrent neural network</a> to understand these concepts:</p>
</blockquote>

<p>Modeling sequential data using coventional <a href="https://emmanuel-arize.github.io/datascience-blog/deeplearning/deep-learning/2021/05/06/RNN.html" target="_blank">  recurrent neural network</a>, sometimes encounter sequences in which the gap between the relevant information and the point where it’s needed is very large, with these kind of huge gaps, RNNs are unable connectinformation to where it’s needed. During the backpropagation phase of RNNs, in which error signals (gradients) are backpropagated through time, the recurrent hidden layers (weight matrix associated with the layers) are subject to repeated multiplications. These multiplications are determined by the number of timesteps (length of the sequence), and this might result in numerical instability for lengthy sequence. For lengthy sequence, small weights tend to lead to a situation known as <b>vanishing gradients</b> where error signals propagating backwards get so small that learning either becomes very slow or stops working altogether (error signals fowing backwards in time tend to vanish). Conversely larger weights tend to lead to a situation where error signals are so large that they can cause learning to diverge, a situation known as <b>exploding gradients</b>.</p>

<p>To read more on exploding and vanishing gradients have a look at this papers
<br />
<a href="https://arxiv.org/pdf/1211.5063v1.pdf" target="_blank">Understanding the exploding gradient problem</a><br />
<a href="https://www.semanticscholar.org/paper/Learning-long-term-dependencies-with-gradient-is-Bengio-Simard/d0be39ee052d246ae99c082a565aba25b811be2d" target="_blank">Learning long-term dependencies with gradient descent is difficult</a><br /></p>

<p><a href="https://www.bioinf.jku.at/publications/older/2304.pdf" target="_blank">THE VANISHING GRADIENT PROBLEM DURING LEARNING RECURRENT NEURAL NETS AND PROBLEM SOLUTIONS</a><br /></p>

<p>The vanishing and exploding gradients problem associated with conventional RNNs , limit their abilities when modeling sequences with long range contextual dependencies and to address these issues, more complex RNNs architectures known as Gated Neural Networks (GNNs) have been designed to mitigate these problems by introducing <strong><em>“Gating Mechanism”</em></strong>  to control the flow of information in and out of the units that comprise the network layers. There are several GNNs but in this tutorial we will learn about a notable example known Long short-term memory (LSTM) networks (<a href="http://www.bioinf.jku.at/publications/older/2604.pdf" target="_blank">Hochreiter and Schmidhuber, 1997</a>)</p>

<h1 id="long-short-term-memory-networks-lstms">Long Short-Term Memory NETWORKS (LSTMs)</h1>

<p>LSTM are design to remember information for long periods of time and this is acheived through the use of a <b>memory cell state denoted by \(C_{t}\) </b> which is controled by the gating mechanism. At each time step, the controllable gating mechanisms decide which parts of the inputs will be written to the memory cell state, and which parts of memory cell state will be overwritten (forgotten), regulating information flowing in and out of the memory cell state and this make LSTMs divide the context management problem into two sub-problems: removing information no longer needed from the context and adding information likely to be needed for later decision making to the context. <a href="#lstm">Figure 1</a>  is a A schematic diagram of LSTMs.</p>

<p><img img="" id="lstm" class="w3-center" src="/arizeblog/assets/images/deep/keras/LSTM.png" /><span id="Fig">Figure 1</span>
<a href="https://www.researchgate.net/figure/Structure-of-the-LSTM-cell-and-equations-that-describe-the-gates-of-an-LSTM-cell_fig5_329362532" target="_blank">source <a></a></a></p>

<p>From <a href="#lstm">Figure 1,</a> the first step of the LSTM model is to decide  how to
 to reset the content of the memory cell and this is controlled by the <b>forget gate </b> denoted as \(f_t\) and defined as</p>

\[f_{t}=\sigma(x_{t}U^{f} +h_{t-1}W^{f} )\]

<p>where \(W^{f}\) denotes the hidden to hidden weights with  the superscript \(f\) as a symbol indicating the forget gate, \(U^{f}\) denoting input to hidden weights. The forget gate computes the weighted sum of the previous hidden state \(h_{t−1}\) and the current input \(x_{t}\) of time step t (time steps correspond to word positions in a sentence) then passes it through a sigmoid activation function which output a vector with values between 0 and 1.  <strong><em>The forgate gate is then multiplied by the previous memory cell \(C_{t-1}\) to decide how much of the previous memory cell content to retain when computing the current memory cell state \(C_{t}\). With a forgate gate value of 0, content of the previous memory cell will be completely discarded and with a value of 1, content of the previous memory cell will be  used when computing the current memory cell</em></strong>. 
Let defined this multiplcation as</p>

\[k_{t}=C_{t-1} \odot f_{t}\]

<blockquote>
  <p>NOTE \(\odot\) the Hadamard product (also known as the element-wise product)</p>
</blockquote>

<p>The next step is to compute the actual information (create a contextual vector or Candidate Memory Cell \(C^{t}\) needed to extract from the previous hidden state and current inputs and is defined by</p>

\[\tilde{C_{t}} = tanh(U^{g}x_{t} + W^{g}h_{t−1} )\]

<p><b>NB:</b>
This is a contextual vector \(\tilde{C_{t}}\) containing all possible values that needs to be
added to the cell state.</p>

<p>The model then decide how information stored in the Candidate Memory Cell is selected and this is regulated by the <b> add or input gate</b>. The input gate is defined as</p>

\[i_{t} = \sigma(U^{i}x_{t} +W^{i}h_{t−1})\]

<p>The <b>input gate</b> then select information needed to be added to the current memory cell state via Candidate Memory Cell and is defined as</p>

\[j_{t} = \tilde{C_{t}}\odot i_{t}\]

<p>we now defined the current memory cell state \(C_{t}\) as</p>

\[C_{t}=k_{t}+j_{t}=C_{t-1} \odot f_{t}  + \tilde{C_{t}}\odot i_{t}\]

<p><b>NB :</b> This is the Cell state that stores information and is responsible for remembering information for long period of time</p>

<p>Not all information stored in the current memory cell state is required for the current hidden state, so the <b>output gate</b> then decides information required for the current hidden state and is defined as</p>

\[o_{t} = \sigma(U^{o}x_{t} +W^{o}h_{t−1})\]

<p>The current hidden state $h_{t}$ is then defined as</p>

\[h_{t}=o_{t} \odot tanh(C_{t})\]

<h1 id="let-now-implement-the-model-using-keras">Let now implement the model using keras</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span> <span class="k">as</span> <span class="n">K</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers.experimental.preprocessing</span> <span class="kn">import</span> <span class="n">TextVectorization</span>
</code></pre></div></div>

<p>Loading data</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch_size</span><span class="o">=</span><span class="mi">124</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">train_data</span><span class="o">=</span><span class="n">K</span><span class="p">.</span><span class="n">preprocessing</span><span class="p">.</span><span class="n">text_dataset_from_directory</span><span class="p">(</span><span class="n">directory</span><span class="o">=</span><span class="s">'./data/stackoverflow/train'</span><span class="p">,</span>
                                                 <span class="n">subset</span><span class="o">=</span><span class="s">'training'</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                                       <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
                                                 <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
<span class="n">val_data</span><span class="o">=</span><span class="n">K</span><span class="p">.</span><span class="n">preprocessing</span><span class="p">.</span><span class="n">text_dataset_from_directory</span><span class="p">(</span><span class="n">directory</span><span class="o">=</span><span class="s">'./data/stackoverflow/train'</span><span class="p">,</span>
                                                 <span class="n">subset</span><span class="o">=</span><span class="s">'validation'</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
                                                 <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
<span class="n">test_data</span><span class="o">=</span><span class="n">K</span><span class="p">.</span><span class="n">preprocessing</span><span class="p">.</span><span class="n">text_dataset_from_directory</span><span class="p">(</span><span class="n">directory</span><span class="o">=</span><span class="s">'./data/stackoverflow/test'</span><span class="p">,</span>
                                             <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span> <span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Found 8000 files belonging to 4 classes.
Using 6000 files for training.
Found 8000 files belonging to 4 classes.
Using 2000 files for validation.
Found 8000 files belonging to 4 classes.
</code></pre></div></div>

<p>From the above result, there are 8000 examples of which 75% representing 6000 is used as the training set and 25% (2000) as a validation set. The data has a label categorize into 4 classes</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_data</span><span class="p">.</span><span class="n">class_names</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'index'</span> <span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="s">"corresponds to "</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>index 0 corresponds to  csharp
index 1 corresponds to  java
index 2 corresponds to  javascript
index 3 corresponds to  python
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">.</span><span class="n">take</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">numpy</span><span class="p">()[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">,</span><span class="n">y</span><span class="p">.</span><span class="n">numpy</span><span class="p">()[</span><span class="n">i</span><span class="p">],</span><span class="s">'--'</span><span class="p">,</span><span class="n">train_data</span><span class="p">.</span><span class="n">class_names</span><span class="p">[</span><span class="n">i</span><span class="p">],)</span>
        <span class="k">break</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>b'"blank boolean expression for a string in do-while loop public class studentgrades {..    string studentid;.    integer numericgrade;..    scanner input = new scanner(system.in);..public void loadstudentgrades(){.    do{.        system.out.println(""please enter a student id, or enter \'end\' to exit: "");.        studentid = input.next();.        system.out.println(""please enter numeric grade for the id above: "");.        numericgrade = input.nextint();.        map.put(studentid, numericgrade);.        }.    while (studentid !string.equals(""end"")); //this is throwing an error. how is it possible to get this to work?.    }.}...i\'m working on this class and am finding it difficult to get the while part of my do-while loop to work the way i was expecting it to. i want to say while studentid is not equal to ""end"" to go through the loop."\n'

 1 -- csharp
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_data</span><span class="o">=</span><span class="n">train_data</span><span class="p">.</span><span class="n">cache</span><span class="p">().</span><span class="n">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>
<span class="n">val_data</span><span class="o">=</span><span class="n">val_data</span><span class="p">.</span><span class="n">cache</span><span class="p">().</span><span class="n">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>
<span class="n">test_data</span><span class="o">=</span><span class="n">test_data</span><span class="p">.</span><span class="n">cache</span><span class="p">().</span><span class="n">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">process_data</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">lower_data</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">strings</span><span class="p">.</span><span class="n">lower</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">lower_data</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">strings</span><span class="p">.</span><span class="n">strip</span><span class="p">(</span><span class="n">lower_data</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">strings</span><span class="p">.</span><span class="n">regex_replace</span><span class="p">(</span><span class="n">lower_data</span><span class="p">,</span><span class="s">"&lt;b /&gt;"</span><span class="p">,</span><span class="s">' '</span><span class="p">)</span>
   
<span class="n">sequence_length</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">encoder</span><span class="o">=</span><span class="n">TextVectorization</span><span class="p">(</span><span class="n">max_tokens</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">output_mode</span><span class="o">=</span><span class="s">'int'</span><span class="p">,</span><span class="n">standardize</span><span class="o">=</span><span class="n">process_data</span><span class="p">,</span>
                          <span class="n">output_sequence_length</span><span class="o">=</span><span class="n">sequence_length</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">encoder</span><span class="p">.</span><span class="n">adapt</span><span class="p">(</span><span class="n">train_data</span><span class="p">.</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">:</span><span class="n">x</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LSTM</span><span class="p">(</span><span class="n">K</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">input_encoder</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LSTM</span><span class="p">,</span><span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">input_encoder</span><span class="o">=</span><span class="n">input_encoder</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">embed</span><span class="o">=</span><span class="n">K</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span><span class="n">input_length</span><span class="o">=</span><span class="n">sequence_length</span><span class="p">,</span>
                                      <span class="n">output_dim</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lstm_layer</span><span class="o">=</span><span class="n">K</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="n">recurrent_dropout</span><span class="o">=</span><span class="p">.</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">flat</span><span class="o">=</span><span class="n">K</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="o">=</span><span class="n">K</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dense</span><span class="o">=</span><span class="n">K</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">x</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">input_encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">h</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">lstm_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">h</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">flat</span><span class="p">(</span><span class="n">h</span><span class="p">))</span>
        <span class="n">output</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dense</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
        
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lstm_model</span><span class="o">=</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_encoder</span><span class="o">=</span><span class="n">encoder</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lstm_model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">'rmsprop'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">'sparse_categorical_crossentropy'</span><span class="p">,</span><span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'acc'</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lstm_model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span><span class="n">validation_data</span><span class="o">=</span><span class="n">val_data</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch 1/20
49/49 [==============================] - 16s 231ms/step - loss: 1.3839 - acc: 0.2566 - val_loss: 1.3815 - val_acc: 0.2735
Epoch 2/20
49/49 [==============================] - 8s 162ms/step - loss: 1.3604 - acc: 0.3101 - val_loss: 1.2764 - val_acc: 0.4270
Epoch 3/20
49/49 [==============================] - 8s 157ms/step - loss: 1.2614 - acc: 0.4029 - val_loss: 1.1663 - val_acc: 0.4500
Epoch 4/20
49/49 [==============================] - 9s 182ms/step - loss: 1.1564 - acc: 0.4484 - val_loss: 1.1934 - val_acc: 0.4285
Epoch 5/20
49/49 [==============================] - 8s 155ms/step - loss: 1.1034 - acc: 0.4842 - val_loss: 1.0685 - val_acc: 0.4845
Epoch 6/20
49/49 [==============================] - 8s 161ms/step - loss: 1.0649 - acc: 0.4994 - val_loss: 1.0873 - val_acc: 0.4920
Epoch 7/20
49/49 [==============================] - 8s 163ms/step - loss: 1.0293 - acc: 0.5187 - val_loss: 1.0622 - val_acc: 0.5055
Epoch 8/20
49/49 [==============================] - 8s 169ms/step - loss: 1.0013 - acc: 0.5511 - val_loss: 1.0052 - val_acc: 0.5310
Epoch 9/20
49/49 [==============================] - 8s 160ms/step - loss: 0.9938 - acc: 0.5615 - val_loss: 1.0364 - val_acc: 0.5315
Epoch 10/20
49/49 [==============================] - 8s 157ms/step - loss: 0.9676 - acc: 0.5777 - val_loss: 1.0216 - val_acc: 0.5515
Epoch 11/20
49/49 [==============================] - 8s 159ms/step - loss: 0.9291 - acc: 0.5861 - val_loss: 1.0288 - val_acc: 0.5535
Epoch 12/20
49/49 [==============================] - 8s 159ms/step - loss: 0.8900 - acc: 0.6114 - val_loss: 1.0212 - val_acc: 0.5575
Epoch 13/20
49/49 [==============================] - 8s 163ms/step - loss: 0.8357 - acc: 0.6408 - val_loss: 1.1499 - val_acc: 0.5635
Epoch 14/20
49/49 [==============================] - 8s 159ms/step - loss: 0.8424 - acc: 0.6491 - val_loss: 1.0042 - val_acc: 0.5900
Epoch 15/20
49/49 [==============================] - 9s 174ms/step - loss: 0.7966 - acc: 0.6664 - val_loss: 0.8653 - val_acc: 0.6285
Epoch 16/20
49/49 [==============================] - 9s 176ms/step - loss: 0.7437 - acc: 0.6879 - val_loss: 0.9403 - val_acc: 0.6265
Epoch 17/20
49/49 [==============================] - 8s 171ms/step - loss: 0.7550 - acc: 0.6924 - val_loss: 0.9791 - val_acc: 0.6265
Epoch 18/20
49/49 [==============================] - 9s 183ms/step - loss: 0.7420 - acc: 0.7027 - val_loss: 0.9143 - val_acc: 0.6355
Epoch 19/20
49/49 [==============================] - 9s 176ms/step - loss: 0.7129 - acc: 0.7033 - val_loss: 0.9438 - val_acc: 0.6395
Epoch 20/20
49/49 [==============================] - 9s 185ms/step - loss: 0.7122 - acc: 0.7035 - val_loss: 1.0339 - val_acc: 0.6300





&lt;tensorflow.python.keras.callbacks.History at 0x2cfc07b22e8&gt;
</code></pre></div></div>

<p>let evaluate our model on the test set</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lstm_model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>65/65 [==============================] - 13s 172ms/step - loss: 1.0457 - acc: 0.6405





[1.0457446575164795, 0.640500009059906]
</code></pre></div></div>

<p>Let use the model to predict the sample below from the test set.</p>

<p>according the text data the label for the samples are as follows:</p>

<table>
  <thead>
    <tr>
      <th>sample</th>
      <th>Label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>sample 1</td>
      <td>python</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>sample 2</td>
      <td>javascript</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>sample 3</td>
      <td>java</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>sample 4</td>
      <td>python</td>
    </tr>
  </tbody>
</table>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sample</span><span class="o">=</span><span class="p">[</span><span class="s">"variables keep changing back to their original value inside a while loop i am doing the mitx 6.00.01x course and i am on the second problem set on the 3rd problem and i am stuck. .my code:  ..    balance = 320000.    annualinterestrate = 0.2.    monthlyinterestrate = (annualinterestrate) / 12.0.    monthlyfixedpayment = 0.    empbalance = balance.    lowerbound = round((balance)/12,2).    upperbound = (balance*(1+monthlyinterestrate)**12)/12.    monthlyfixedpayment = round( ( (lowerbound+upperbound)/2) ,2).    while tempbalance != 0: .        monthlyfixedpayment = round( ( (lowerbound+upperbound)/2) ,2)  .        for m in range(12) :.            tempbalance -= monthlyfixedpayment .            tempbalance += (monthlyinterestrate)*(tempbalance).            tempbalance = round(tempbalance,2) .        if tempbalance &amp;gt; 0:.            lowerbound = round(monthlyfixedpayment,2).            tempbalance = balance.        elif tempbalance &amp;lt; 0: .            upperbound = round(monthlyfixedpayment,2).            tempbalance = balance..    print('lowest payment: ' + str(round(monthlyfixedpayment,2)))...my code uses bisection search to generate the monthlyfixedpayment but after i get to the lines at the end that changes the upperbound or lowerbound values and then start the loop again, the lowerbound and upperbound values reset to their values to the ones outside the loop. does anyone knows how to prevent this?"</span><span class="p">,</span>
        <span class="s">"how pass window handler from one page to another? (blank) i have a very strange problem , please donâ€™t ask me why do i need thisâ€¦.i have a page1. page1 has a link which opens new window (page2) using  window.open function..chatwindow is a handler of child window with returns from window.open function..now i'm moving from page1 to page3 (by link &amp;lt;a href=""...."" target=""_self""&amp;gt;some text&amp;lt;/a&amp;gt;). and i need to check on the page3 if page2 is close or open..how to pass handler chatwindow from page1 to page3?..thank you in advance!"</span><span class="p">,</span>
        <span class="s">"what is the difference between text and string? in going through the blankfx tutorial i've run into the text, and it's being used where i would have thought a string would be used. is the only difference between..string foo = new string(""bat"");...and..text bar = new text(""bat"");...that bar cannot be edited, or are there other differences that i haven't been able to find in my research?"</span><span class="p">,</span>
        <span class="s">"idiomatic blank iterating and adding to a dict i'm running through a string, creating all substrings of size 10, and adding them to a dict. this is my code,..sequence_map = {}.for i in range(len(s)):.    sub = s[i:i+10].    if sub in sequence_map:.       sequence_map[sub] += 1.    else:.       sequence_map[sub] = 1...is there a way to do this more blankically?..also how do i do the reverse blankically, as in interating through the dict and composing a list where value is equal to something?..[k for k, v in sequence_map.items()]"</span>
<span class="p">]</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">result</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">lstm_model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">sample</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">result</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([2, 2, 1, 3], dtype=int64)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pred</span><span class="p">(</span><span class="n">result</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'csharp'</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">i</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'java'</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">i</span><span class="o">==</span><span class="mi">2</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'javascript'</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">i</span><span class="o">==</span><span class="mi">3</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'python'</span><span class="p">)</span>
<span class="n">pred</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>javascript
javascript
java
python
</code></pre></div></div>

<p>Reference</p>

<p><a href="https://web.stanford.edu/~jurafsky/slp3/9.pdf">Deep Learning Architectures
for Sequence Processing</a><br /></p>

<p><a href="https://www.deeplearningbook.org/contents/rnn.html">
Ian Goodfellow, Yoshua Bengio and Aaron Courville (2016). Deep Learning. MIT Press,pp.389-413
</a></p>

<p><a href="https://link.springer.com/article/10.1007/BF00114844">Elman, J. L. (1991). Distributed representations, simple recurrent networks, and grammatical structure. Machine learning, 7(2), 195-225.</a><br /></p>

<p><a href="https://arxiv.org/pdf/1412.7753.pdf">Mikolov, T., Joulin, A., Chopra, S., Mathieu, M., &amp; Ranzato, M. A. (2014). Learning longer memory in recurrent neural networks. arXiv preprint arXiv:1412.7753.</a><br /></p>

<p><a href="https://www.morganclaypool.com/doi/abs/10.2200/S00762ED1V01Y201703HLT037">
Neural Network Methods for Natural Language Processing Synthesis Lectures on Human Language Technologies</a><br /></p>

<p><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a><br /></p>

<p><a href=""></a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>]]></content><author><name>Arize Emmanuel</name></author><category term="machine-learning-python" /><summary type="html"><![CDATA[In this post, I will explain the internal mechanisms that allow LSTM networks to perform better when model temporal sequences and their long-range dependencies than the coventional RNNs, We will then use it in real life problem by training LSTM as a multi-class classifier to predict the tag of a programming question on Stack Overflow using Tensorflow/Keras.]]></summary></entry><entry><title type="html">Introduction to Recurrent Neural Networks (RNNs)</title><link href="http://localhost:4000/arizeblog/machine-learning-python/2021/05/06/SIMPLE_RNN.html" rel="alternate" type="text/html" title="Introduction to Recurrent Neural Networks (RNNs)" /><published>2021-05-06T00:00:00+00:00</published><updated>2021-05-06T00:00:00+00:00</updated><id>http://localhost:4000/arizeblog/machine-learning-python/2021/05/06/SIMPLE_RNN</id><content type="html" xml:base="http://localhost:4000/arizeblog/machine-learning-python/2021/05/06/SIMPLE_RNN.html"><![CDATA[<p>A major characteristic of feedforward networks is that these networks take in arbitrary feature vectors with fixed, predetermined input sizes, along with their associated weights and had no hidden state. With feedforward networks, in order to process a sequence of data points, the entire sequence is considered as a single input for the network to process and capture all relevant information at once in a single step. This makes it difficult to deal with sequences of varying length and fails to capture important information. Sequential data usually involve variable lenght inputs, so instead of processing the data point in just a single step, we need a model that will still consider a sequence as a single input to the network but instead of processing it in a single step, ita will internally loop over the sequence elements taking each element as an input and maintaining a state containing information relative to what it has seen so far and this is the idea behind RNNS.</p>

<p><b style="text-decoration:underline;font-size: 20px;text-transform: uppercase;">Recurrent neural networks or RNNs</b> are networks containing recurrent connections within their network connections and are often used for processing sequential data.</p>

<p>An assumption of feedforward networks is that the inputs are independent (one input has no dependency on another) of one another, however in sequential data such as textual data (we will limit ourselves to textual data since it is the most widespread forms of sequence data), this assumption is not true, since in a sentence the occurrence of a word influences or is influenced by the occurrences of other words in the sentence.</p>

<p>When processing data, RNNs assumes that an incoming data take the form of a sequence of vectors or tensors, which can be sequences of word or sequences of characters as in textual data, sequence of observations over a period of time as in time series etc.  As RNNs process the data, RNNs have recurrent connections that allows a memory to persist in the network’s internal state keeping track of information observed so far and informing the decisions to be made by the network at later points in time and also share parameters across different parts of the network making it possible to extend and apply the network to inputs of variable lengths and generalize across them. This makes RNNs useful for Timeseries forecasting and natural language processing (NLP) systems such as document classification, sentiment analysis, automatic translation, generating text for applications such as chatbots. Since the same parameters are used for all time steps, the parameterization cost of an RNN does not grow as the number of time steps increases.</p>

<p>From the book <a href="https://www.deeplearningbook.org/contents/rnn.html" target="_blank">Deep Learning: Sequence Modeling: Recurrentand Recursive Nets
</a></p>

<blockquote>
  <p>Some examples of important design patterns for recurrent neural networks
include the following:</p>
  <ul>
    <li>Recurrent networks that produce an output at each time step and have
recurrent connections between hidden units.</li>
    <li>Recurrent networks that produce an output at each time step and have
recurrent connections only from the output at one time step to the hidden
units at the next time step.</li>
    <li>Recurrent networks with recurrent connections between hidden units, that
read an entire sequence and then produce a single output.</li>
  </ul>
</blockquote>

<p>In this section, we will  consider a class of recurrent networks referred to as Elman Networks (Elman,1990) or simple recurrent networks which serve as the basis for more complex approaches like the Long Short-Term Memory (LSTM) networks and Gated Recurrent Unit (GRU). A simple RNN which is typically a three-layer network comprising an input layer, a single hidden layer and an output layer.</p>

<p><a style="color:blue;" href="#Fig">Figure 1</a> is a diagrammatic representation of RNN with  input to hidden connections parametrized by a weight matrix \(U \in R^{d \times h}\) which is used to condition the input word vector \(x^{t}\), hidden-to-hidden recurrent connections parametrized by a weight matrix \(W\in R^{h \times h}\), used to condition the output of the previous time-step \(h^{t−1}\), hidden-to-output connections parametrized by a weight matrix  \(V \in R^{h*o}\)
 used to condition the output of the current time-step.</p>

<p>and \(h \in R^{n*h}\) representing the hidden state vector or simply the hidden state (or hidden variable) of the network keeping track or as a loose summary of information observed so far and informing the decisions to be made by the network at later points in time. On the Left side is RNN drawn with recurrent connections and on the Right is the same RNN seen as an time unfolded computational graph, where each node is now associated with one particular time instance and this illustrate the computational logic of an RNN at adjacent time steps.</p>

<p><img class="w3-center" src="/arizeblog/assets/images/deep/keras/rnn.png" />
<br />
<span id="Fig">Figure 1</span><br />
<a href="https://www.google.com/search?q=rnn+image&amp;client=firefox-b-d&amp;tbm=isch&amp;source=iu&amp;ictx=1&amp;fir=lD-kwEF8OCJIoM%252C5nGST21LG70DyM%252C_&amp;vet=1&amp;usg=AI4_-kTE51-vQdo1Mb1V3I10kNw5Xv3yAw&amp;sa=X&amp;ved=2ahUKEwir7rW18sjuAhVOXMAKHSm_CMQQ9QF6BAgHEAE&amp;biw=1366&amp;bih=580#imgrc=8TAzbbCVWa8qZM">source: RNN</a>
<br /></p>

<p>Most RNNs computation  can be decomposed into three blocks of parameters and associated transformations or activation function:</p>
<ul>
  <li>
    <ol>
      <li>from the input to the hidden state,</li>
    </ol>
  </li>
  <li>
    <ol>
      <li>from the previous hidden state to the next hidden state, and</li>
    </ol>
  </li>
  <li>
    <ol>
      <li>from the hidden state to the output</li>
    </ol>
  </li>
</ul>

<p>Armed with a summary of RNNs computational decomposition, let assume we have a minibatch of inputs \(X^{t} \in R^{n×d}\) where each row of \(X^{t}\) corresponds to one example at time step <strong><em>t</em></strong> from the sequence and  \(h^{t} \in R^{n*h}\) be  the hidden state at time <strong><em>t</em></strong>. 
 Unlike standard feedforward networks, RNNs current hidden state \(h^{t}\) is a function \(\phi\) of the previous hidden state \(h^{t-1}\) and the current input \(x^{t}\) defined by \(h^{t}=\phi(x^{t}U+ h^{t-1}W+b^{h} )\) where  \(b^{h} \in R^h\) is the bias term and the weights $W$ determine how the network current state makes use of past context in calculating the output for the current input.</p>

<p>The fact that the computation of the hidden state at time t requires the value of the hidden state from time <strong>t−1</strong> mandates an incremental inference algorithm that proceeds from the start of the sequence to the end and thus RNNs condition the next word on the sentence history. With \(h^{t}\) defined, RNN is defined as function $\phi$ taking as input a state vector \(h^{t-1}\) and an input vector \(x^{t}\) and return a new state vector \(h^{t}\). The initial state vector \(h^{0}\), is also an input to RNN, assumed to be a zero vector and often omitted. The hidden state $h$ is then used as the input for the output layer and is given by</p>

\[h^{t}=\phi(X^{t}U+ h^{t-1}W+b^{h} )\]

\[O=f(h^{t}V +b^{o})\]

\[\hat y=\phi (O)\]

<p>where \(O \in R^{n \times o}\),  \(b^{o} \in R^{o}\) and \(\hat y\) is the next predicted word given the current hidden state.</p>

<p>Layers performing \(h^{t}=\phi(x^{t}U+ h^{t-1}W+b^{h} )\)  in RNNs are called recurrent layers.</p>

<h1 id="let-us-now-train-a-simple-rnn-to-predict-a-movie-review">Let us now train a simple RNN to predict a movie review</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span> <span class="k">as</span> <span class="n">K</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers.experimental.preprocessing</span> <span class="kn">import</span> <span class="n">TextVectorization</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">import</span> <span class="nn">os</span><span class="p">,</span><span class="n">re</span><span class="p">,</span><span class="n">string</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
</code></pre></div></div>

<h1 id="loading-the-imdb-data">Loading the IMDB data</h1>
<p>You’ll restrict the movie reviews to the top 15,000 most common words and  considering looking at the first 30 words in every review. The network will learn 16-dimensional embeddings for each of the 15,000 words</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch_size</span><span class="o">=</span><span class="mi">100</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">train_data</span><span class="o">=</span><span class="n">K</span><span class="p">.</span><span class="n">preprocessing</span><span class="p">.</span><span class="n">text_dataset_from_directory</span><span class="p">(</span><span class="n">directory</span><span class="o">=</span><span class="s">'./data/aclImdb/train/'</span><span class="p">,</span>
                                                               <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                                               <span class="n">subset</span><span class="o">=</span><span class="s">'training'</span><span class="p">,</span>
                                                              <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
                                                               <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="n">val_data</span><span class="o">=</span><span class="n">K</span><span class="p">.</span><span class="n">preprocessing</span><span class="p">.</span><span class="n">text_dataset_from_directory</span><span class="p">(</span><span class="n">directory</span><span class="o">=</span><span class="s">'./data/aclImdb/train/'</span><span class="p">,</span>
                                                               <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                                            <span class="n">subset</span><span class="o">=</span><span class="s">'validation'</span><span class="p">,</span>
                                                              <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
                                                            <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Found 25000 files belonging to 2 classes.
Using 18750 files for training.
Found 25000 files belonging to 2 classes.
Using 6250 files for validation.
</code></pre></div></div>

<p>From the above result, there are 25,000 examples in the training folder, of which 75% (18750) is used for training and 25% (6250) as a validation set</p>

<p>names of categories under the labels</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_data</span><span class="p">.</span><span class="n">class_names</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'index'</span> <span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="s">"corresponds to "</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>index 0 corresponds to  neg
index 1 corresponds to  pos
</code></pre></div></div>

<p>Creates a <code class="language-plaintext highlighter-rouge">Dataset</code> that prefetches elements from this dataset</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">AUTOTUNE</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">AUTOTUNE</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">train_data</span><span class="o">=</span><span class="n">train_data</span><span class="p">.</span><span class="n">cache</span><span class="p">().</span><span class="n">prefetch</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="n">AUTOTUNE</span><span class="p">)</span>
<span class="c1">#val_data=val_data.cache().prefetch(buffer_size=AUTOTUNE)
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">.</span><span class="n">take</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">numpy</span><span class="p">(),</span><span class="s">'</span><span class="se">\n</span><span class="s"> </span><span class="se">\n</span><span class="s">'</span><span class="p">,</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">numpy</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>b"Neither the total disaster the UK critics claimed nor the misunderstood masterpiece its few fanboys insist, Revolver is at the very least an admirable attempt by Guy Ritchie to add a little substance to his conman capers. But then, nothing is more despised than an ambitious film that bites off more than it can chew, especially one using the gangster/con-artist movie framework. As might be expected from Luc Besson's name on the credits as producer, there's a definite element of 'Cinema de look' about it: set in a kind of realistic fantasy world where America and Britain overlap, it looks great, has a couple of superbly edited and conceived action sequences and oozes style, all of which mark it up as a disposable entertainment. But Ritchie clearly wants to do more than simply rehash his own movies for a fast buck, and he's spent a lot of time thinking and reading about life, the universe and everything. If anything its problem is that he's trying to throw in too many influences (a bit of Machiavelli, a dash of Godard, a lot of the Principles of Chess), motifs and techniques, littering the screen with quotes: the film was originally intended to end with three minutes of epigrams over photos of corpses of mob victims, and at times it feels as if he never read a fortune cookie he didn't want to turn into a movie. Rather than a commercial for Kabbalism, it's really more a mixture of the overlapping principles of commerce, chess and confidence trickery that for the most part pulls off the difficult trick of making the theosophy accessible while hiding the film's central (somewhat metaphysical) con.&lt;br /&gt;&lt;br /&gt;The last third is where most of the problems can be found as Jason Statham takes on the enemy (literally) within with lots of ambitious but not always entirely successful crosscutting within the frame to contrast people's exterior bravado with their inner fear and anger, but it's got a lot going for it all the same. Not worth starting a new religion over, but I'm surprised it didn't get a US distributor. Maybe they found Ray Liotta's intentionally fake tan just too damn scary?" 
 
 1
</code></pre></div></div>

<h1 id="text-preprocessing">Text preprocessing</h1>

<p>You’ll restrict the movie reviews to the top 15,000 most common words and will consider looking at the first 30 words in every review. The network will learn 16-dimensional embeddings for each of the 15,000 words</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">max_features</span><span class="o">=</span><span class="mi">15000</span>
<span class="n">max_len</span><span class="o">=</span><span class="mi">30</span>
<span class="n">encoder</span><span class="o">=</span><span class="n">TextVectorization</span><span class="p">(</span><span class="n">max_tokens</span><span class="o">=</span><span class="n">max_features</span><span class="p">,</span><span class="n">output_mode</span><span class="o">=</span><span class="s">"int"</span><span class="p">,</span>
                          <span class="n">output_sequence_length</span><span class="o">=</span><span class="n">max_len</span><span class="p">,</span>
         <span class="n">standardize</span><span class="o">=</span><span class="s">'lower_and_strip_punctuation'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">encoder</span><span class="p">.</span><span class="n">adapt</span><span class="p">(</span><span class="n">train_data</span><span class="p">.</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">:</span><span class="n">x</span><span class="p">))</span>
</code></pre></div></div>

<p>The .adapt method sets the layer’s vocabulary. Here are the first 70 tokens. After the padding and unknown tokens they’re sorted by frequency:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">encoder</span><span class="p">.</span><span class="n">get_vocabulary</span><span class="p">()[:</span><span class="mi">70</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i', 'this', 'that', 'br', 'was', 'as', 'for', 'with', 'but', 'movie', 'film', 'on', 'not', 'you', 'are', 'his', 'have', 'he', 'be', 'one', 'its', 'at', 'all', 'by', 'an', 'they', 'from', 'who', 'so', 'like', 'her', 'just', 'or', 'about', 'has', 'if', 'out', 'some', 'there', 'what', 'good', 'very', 'more', 'when', 'even', 'she', 'no', 'my', 'up', 'would', 'only', 'time', 'which', 'really', 'story', 'their', 'were', 'had', 'see', 'can']
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">len</span><span class="p">(</span><span class="n">encoder</span><span class="p">.</span><span class="n">get_vocabulary</span><span class="p">())</span><span class="o">==</span><span class="n">max_features</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>True
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RNNs</span><span class="p">(</span><span class="n">K</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">embedd_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RNNs</span><span class="p">,</span><span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">embedd_dim</span><span class="o">=</span><span class="n">embedd_dim</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">embedded_layer</span><span class="o">=</span><span class="n">K</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">max_features</span><span class="p">,</span><span class="n">input_length</span><span class="o">=</span><span class="n">max_len</span><span class="p">,</span>
                                               <span class="n">output_dim</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">embedd_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">rnn</span><span class="o">=</span><span class="n">K</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">SimpleRNN</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">f</span><span class="o">=</span><span class="n">K</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="o">=</span><span class="n">K</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dense</span><span class="o">=</span><span class="n">K</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">encoded_input</span><span class="o">=</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">embed</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">embedded_layer</span><span class="p">(</span><span class="n">encoded_input</span><span class="p">)</span>
        <span class="n">rnn_layer</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">embed</span><span class="p">)</span>
        <span class="n">dropout_layer</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">rnn_layer</span><span class="p">)</span>
        <span class="n">output</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dense</span><span class="p">(</span><span class="n">dropout_layer</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">embedding_dim</span><span class="o">=</span><span class="mi">16</span>

<span class="n">model</span><span class="o">=</span><span class="n">RNNs</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'rmsprop'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'acc'</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span><span class="n">validation_data</span><span class="o">=</span><span class="n">val_data</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch 1/6
188/188 [==============================] - 28s 116ms/step - loss: 0.6931 - acc: 0.5087 - val_loss: 0.6920 - val_acc: 0.5302
Epoch 2/6
188/188 [==============================] - 15s 80ms/step - loss: 0.6756 - acc: 0.5999 - val_loss: 0.6030 - val_acc: 0.6744
Epoch 3/6
188/188 [==============================] - 16s 87ms/step - loss: 0.5464 - acc: 0.7315 - val_loss: 0.5255 - val_acc: 0.7392
Epoch 4/6
188/188 [==============================] - 16s 87ms/step - loss: 0.4493 - acc: 0.8014 - val_loss: 0.5192 - val_acc: 0.7443
Epoch 5/6
188/188 [==============================] - 18s 93ms/step - loss: 0.3941 - acc: 0.8296 - val_loss: 0.5410 - val_acc: 0.7307
Epoch 6/6
188/188 [==============================] - 17s 89ms/step - loss: 0.3551 - acc: 0.8481 - val_loss: 0.5651 - val_acc: 0.7226





&lt;tensorflow.python.keras.callbacks.History at 0x1b52dae1dd8&gt;
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">text</span> <span class="o">=</span> <span class="p">[</span>
     <span class="s">"This movie is fantastic! I really like it because it is so good!"</span><span class="p">,</span>
    <span class="s">"Not a good movie!"</span><span class="p">,</span>
    <span class="s">"The movie was great!"</span><span class="p">,</span>
    <span class="s">"This movie really sucks! Can I get my money back please?"</span><span class="p">,</span>
    <span class="s">"The movie was terrible..."</span><span class="p">,</span>
    <span class="s">"This is a confused movie."</span><span class="p">,</span>
  <span class="s">"The movie is one of best movies i have ever watched!"</span><span class="p">,</span>
  <span class="s">"Men i dont think i can watch this movie again"</span>
    
<span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">text</span><span class="p">:</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span><span class="p">]))</span>
    <span class="n">result</span><span class="o">=</span><span class="s">'positive review'</span> <span class="k">if</span> <span class="n">predictions</span><span class="o">&gt;</span><span class="mf">0.5</span> <span class="k">else</span> <span class="s">'negative review'</span>
    <span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>positive review
positive review
positive review
negative review
negative review
negative review
positive review
negative review
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<p> <b>References:</b></p>

<p><a href="https://web.stanford.edu/~jurafsky/slp3/9.pdf" title="An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition by Daniel Jurafsky &amp; James H. Martin Chapter 9" target="_blank">Deep Learning Architectures for Sequence Processing</a></p>

<p><a href="https://www.tensorflow.org/text/guide/word_embeddings" title="Tensorflow Documentation" target="_blank">Word embeddings</a></p>

<p><a href="https://d2l.ai/chapter_recurrent-neural-networks/rnn.html#equation-rnn-h-with-state" title="Dive Into Deep Learning Chapter8 section-8.4." target="_blank"> Recurrent Neural Networks</a>
<a href="https://www.deeplearningbook.org/contents/rnn.html" title="Ian Goodfellow, Yoshua Bengio and Aaron Courville (2016). Deep Learning.  MIT Press,pp.389-413" target="_blank">Chapter 10:  Sequence Modeling: Recurrentand Recursive Nets</a></p>

<p><a href="https://link.springer.com/article/10.1007/BF00114844" title="Elman, J. L. (1991)." target="_blank">Distributed representations, simple recurrent networks, and grammatical structure. 
Machine learning, 7(2), 195-225.</a></p>

<p><a href="https://arxiv.org/pdf/1412.7753.pdf" title="Mikolov, T., Joulin, A., Chopra, S., Mathieu, M., &amp; Ranzato, M. A. (2014)." target="_blank">Learning longer memory in recurrent neural networks. arXiv preprint arXiv:1412.7753.</a></p>]]></content><author><name>Arize Emmanuel</name></author><category term="machine-learning-python" /><summary type="html"><![CDATA[A major characteristic of feedforward networks is that these networks take in arbitrary feature vectors with fixed, predetermined input sizes, along with their associated weights and had no hidden state. With feedforward networks, in order to process a sequence of data points, the entire sequence is considered as a single input for the network to process and capture all relevant information at once in a single step. This makes it difficult to deal with sequences of varying length and fails to capture important information. Sequential data usually involve variable lenght inputs, so instead of processing the data point in just a single step, we need a model that will still consider a sequence as a single input to the network but instead of processing it in a single step, ita will internally loop over the sequence elements taking each element as an input and maintaining a state containing information relative to what it has seen so far and this is the idea behind RNNS.]]></summary></entry><entry><title type="html">Feature Extraction with Bag of Words (BOWs)</title><link href="http://localhost:4000/arizeblog/machine-learning-python/2021/03/22/BOWS.html" rel="alternate" type="text/html" title="Feature Extraction with Bag of Words (BOWs)" /><published>2021-03-22T00:00:00+00:00</published><updated>2021-03-22T00:00:00+00:00</updated><id>http://localhost:4000/arizeblog/machine-learning-python/2021/03/22/BOWS</id><content type="html" xml:base="http://localhost:4000/arizeblog/machine-learning-python/2021/03/22/BOWS.html"><![CDATA[<p>In this tutorial, we will learn about Bag of Words  (BOWs), how BOWs is used as a feature extractor, then build a classifier using the features extracted.</p>

<p>In order to model a text documents, the raw text cannot be fed directly to the algorithm as these algorithms expect numerical feature vectors so instead we need to turn the text content into numerical feature vectors.</p>

<p><span class="w3-text-blue"> From the <a href="https://scikit-learn.org/stable/modules/feature_extraction.html">scikit-learn documentation</a>:</span>
<b>
We call vectorization the general process of turning a collection of text documents into numerical feature vectors.
</b></p>

<p>When modelling a data it is important to decide what features of the input are relevant, and how to encode those features. When we consider a textual data such as a sentence or a document  for instance the observable features are the counts and the order of the letters and the words within the text and as such we a way to extract these features from the text. There are several ways of extracting features from a textual data but in this tutorial we will only consider a very common feature extraction procedures for sentences and documents known as the <b> bag-of-words approach (BOW)</b> which looks at the histogram of the unique words within the text ( considering each word count as a feature.)</p>

<p><b>Bag Of Words (BOWs) Approach</b></p>
<p>Is a feature extraction technique used for extracting features from textual data and is commonly used in problems such as language modeling and document classification.  A bag-of-words is a representation of textual data, describing the occurrence of words within a sentence or document, disregarding grammar and the order of words.</p>

<p><b>How does Bag of Words Works</b></p>
<p>In order to understand how bag of words works let consider the two simple text documents:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Boys like playing football and Emma is a boy so Emma likes playing football

2  Mary likes watching movies 

</code></pre></div></div>

<p>Based on these two text documents, a list of token (words) for each document is as follows</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="dl">'</span><span class="s1">Boys</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">like</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">playing</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">football</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">and</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">Emma</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">is</span><span class="dl">'</span> <span class="dl">'</span><span class="s1">a</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">boy</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">so</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">Emma</span><span class="dl">'</span><span class="p">,</span> 

<span class="dl">'</span><span class="s1">likes</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">playing</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">football</span><span class="dl">'</span><span class="p">]</span>


<span class="p">[</span><span class="dl">'</span><span class="s1">Mary</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">likes</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">watching</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">movies</span><span class="dl">'</span><span class="p">]</span>


</code></pre></div></div>
<p>denoting document 1 by doc1 and 2  by doc2, we will construct a dictionary (key-&gt;value pair) of
words for both doc1 and doc2 where each key is a word, and each value is the number of occurrences of that word in the given text document.</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">doc1</span><span class="o">=</span><span class="p">{</span> <span class="dl">'</span><span class="s1">a</span><span class="dl">'</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="dl">'</span><span class="s1">and</span><span class="dl">'</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="dl">'</span><span class="s1">boy</span><span class="dl">'</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="dl">'</span><span class="s1">Boys</span><span class="dl">'</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="dl">'</span><span class="s1">Emma</span><span class="dl">'</span> <span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="dl">'</span><span class="s1">football</span><span class="dl">'</span> <span class="p">:</span> <span class="mi">2</span><span class="p">,</span> 

<span class="dl">'</span><span class="s1">is</span><span class="dl">'</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="dl">'</span><span class="s1">like</span><span class="dl">'</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>  <span class="dl">'</span><span class="s1">likes</span><span class="dl">'</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="dl">'</span><span class="s1">playing</span><span class="dl">'</span> <span class="p">:</span> <span class="mi">2</span><span class="p">,</span>   <span class="dl">'</span><span class="s1">so</span><span class="dl">'</span> <span class="p">:</span> <span class="mi">1</span><span class="p">}</span>

<span class="nx">dco2</span><span class="o">=</span><span class="p">{</span><span class="dl">'</span><span class="s1">likes</span><span class="dl">'</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="dl">'</span><span class="s1">Mary</span><span class="dl">'</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>  <span class="dl">'</span><span class="s1">movies</span><span class="dl">'</span> <span class="p">:</span> <span class="mi">1</span> <span class="p">,</span><span class="dl">'</span><span class="s1">watching</span><span class="dl">'</span> <span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
</code></pre></div></div>

<p><b>NOTE :</b> the order of the words is not important</p>

<p>considering <strong>a</strong> as a stop word, we first define our vocabulary words, which is the set of all unique words found in our document set and it consist of</p>
<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">and</span><span class="p">,</span> <span class="nx">boy</span><span class="p">,</span> <span class="nx">boys</span><span class="p">,</span> <span class="nx">emma</span><span class="p">,</span>  <span class="nx">football</span><span class="p">,</span> <span class="nx">is</span><span class="p">,</span> <span class="nx">like</span><span class="p">,</span> <span class="nx">likes</span><span class="p">,</span> <span class="nx">mary</span><span class="p">,</span> <span class="nx">movies</span><span class="p">,</span> <span class="nx">playing</span><span class="p">,</span> <span class="nx">so</span><span class="p">,</span> <span class="nx">watching</span>

</code></pre></div></div>
<p>and the  features extracted using bag of words for the document set will be</p>

<p><img class=" w3-border" src="/arizeblog/assets/images/python/bog.jpg" /></p>

<p><b>scikit-learn CountVectorize implementation</b></p>
<p>
Using CountVectorize the text is preprocessed, tokenize and stopwords are filtered, it then builds a dictionary of features and transforms documents to feature vectors:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docs=['Boys like playing football and Emma is a boy so Emma likes playing football',
   "Mary likes watching movies"]
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feature_extr</span><span class="o">=</span><span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">model</span><span class="o">=</span><span class="n">feature_extr</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre></div></div>

<p><img class=" w3-border" src="/arizeblog/assets/images/python/bog1.jpg" /></p>

<p><b>Disadvantages</b></p>
<p>Although BOWs is very simple to understand and implement, it has some disadvantages which include</p>

<ul>
  <li>highly sparse vectors or matrix as the are  very few non-zero elements in dimensions corresponding to words that occur in the sentence.</li>
  <li>Bag of words representation leads to a high dimensional feature vector as the total dimension is the vocabulary size.</li>
  <li>Bag of words representation does not consider the semantic relation between words by assuming that the words are independent of each other.</li>
</ul>

<p><b> Buiding a Classifier with the features extracted using BOWS</b></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
</code></pre></div></div>
<p>The dataset called “Twenty Newsgroups”. which is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. <a href="http://qwone.com/~jason/20Newsgroups/">Official description of theTwenty Newsgroups data</a> will be used as our data but instead of the 20 different groups we will work on a partial dataset with only 11 categories out of the 20 available in the dataset. The code below is list of the 11 categories.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">categories</span> <span class="o">=</span> <span class="p">[</span><span class="s">'alt.atheism'</span><span class="p">,</span> <span class="s">'soc.religion.christian'</span><span class="p">,</span><span class="s">'comp.graphics'</span><span class="p">,</span> <span class="s">'sci.med'</span><span class="p">,</span><span class="s">'sci.electronics'</span><span class="p">,</span>
              <span class="s">'sci.space'</span><span class="p">,</span><span class="s">'talk.politics.guns'</span><span class="p">,</span><span class="s">'talk.politics.mideast'</span><span class="p">,</span><span class="s">'talk.politics.misc'</span><span class="p">,</span>
              <span class="s">'talk.religion.misc'</span><span class="p">,</span><span class="s">'misc.forsale'</span><span class="p">]</span>
</code></pre></div></div>

<p>Loading the training data</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">twenty_news_train</span><span class="o">=</span><span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s">'train'</span><span class="p">,</span><span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">,</span><span class="n">remove</span><span class="o">=</span><span class="p">(</span><span class="s">'footers'</span><span class="p">,</span><span class="s">'headers'</span><span class="p">,</span><span class="s">'quotes'</span><span class="p">))</span>
</code></pre></div></div>
<p>Let’s print the third lines of the first loaded file and the first four categories  with the target variable  :</p>

<p><img class=" w3-border" src="/arizeblog/assets/images/python/bog23.jpg" />
<img class=" w3-border" src="/arizeblog/assets/images/python/bog22.jpg" /></p>

<p><b>A Classifier Pipeline </b></p>

<p>In order to make the vectorizer =&gt; classifier easier to work with, let build a pipeline with  a regularized linear models with stochastic gradient descent (SGD) learning as our classifier as below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">news_clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">twenty_news_train</span><span class="p">.</span><span class="n">data</span><span class="p">,</span><span class="n">twenty_news_train</span><span class="p">.</span><span class="n">target</span><span class="p">)</span>
</code></pre></div></div>
<p>Let now train the classifier</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">news_clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">twenty_news_train</span><span class="p">.</span><span class="n">data</span><span class="p">,</span><span class="n">twenty_news_train</span><span class="p">.</span><span class="n">target</span><span class="p">)</span>
</code></pre></div></div>
<p>We now test the classifier on new instances</p>

<p><img class=" w3-border" src="/arizeblog/assets/images/python/bog3.jpg" /></p>

<p><b>Evaluation of the performance on the test set</b></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">twenty_news_test</span><span class="o">=</span><span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s">'test'</span><span class="p">,</span><span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">,</span><span class="n">remove</span><span class="o">=</span><span class="p">(</span><span class="s">'footers'</span><span class="p">,</span><span class="s">'headers'</span><span class="p">,</span><span class="s">'quotes'</span><span class="p">))</span>
</code></pre></div></div>
<p><img class=" w3-border" src="/arizeblog/assets/images/python/bog4.jpg" /></p>

<p> <b>References:</b></p>
<hr />

<ul>
  <li><a href="https://en.wikipedia.org/wiki/Bag-of-words_model" target="_blank">Bag-of-words model - Wikipedia
</a><br /></li>
  <li>
    <p><a href="https://www.amazon.com/Language-Processing-Synthesis-Lectures-Technologies/dp/1627052984/ref=as_li_ss_tl?ie=UTF8&amp;qid=1502062931&amp;sr=8-1&amp;keywords=Neural+Network+Methods+in+Natural+Language+Processing&amp;linkCode=sl1&amp;tag=inspiredalgor-20&amp;linkId=d63df073fea3ebe2d405820570b3ff03" target="_blank">Yoav Goldberg (2017). Neural Network Methods in Natural Language Processing</a><br /></p>
  </li>
  <li><a href="https://scikit-learn.org/stable/modules/feature_extraction.html" target="_blank">Feature extraction-scikit-learn documentation</a><br /></li>
</ul>]]></content><author><name>Emmanuel Arize</name></author><category term="machine-learning-python" /><summary type="html"><![CDATA[In this tutorial, we will learn about Bag of Words (BOWs), how BOWs is used as a feature extractor, then build a classifier using the features extracted.]]></summary></entry><entry><title type="html">Dropout Regularization</title><link href="http://localhost:4000/arizeblog/machine-learning-python/2021/02/10/Dropout.html" rel="alternate" type="text/html" title="Dropout Regularization" /><published>2021-02-10T00:00:00+00:00</published><updated>2021-02-10T00:00:00+00:00</updated><id>http://localhost:4000/arizeblog/machine-learning-python/2021/02/10/Dropout</id><content type="html" xml:base="http://localhost:4000/arizeblog/machine-learning-python/2021/02/10/Dropout.html"><![CDATA[<p>In this article, we will learn about , what it is, how it works and  we will implement it sampling using Python</p>

<p>The term <b>“dropout”</b> refers to dropping out units in a neural network. It is a technique for addressing overfitting. It consists of randomly dropping out some fraction of the nodes (setting fraction of the units to zero (injecting noise)) in each layer before calculating subsequent layer during training and has become a standard technique for training neural networks. When dropout is applied, during training its zeros out some fraction of the nodes with probability p in each layer before calculating the subsequent layer and the resulting network can be viewed as a subset of the original network. Because the fraction of the nodes that are drop out are chosen randomly on every pass, the representations in each layer can’t depend on the exact values taken by nodes in the previous layer.</p>

<p><b> Dropout rate</b> is the fraction of the nodes in a layer that are zeroed out and it’s usually set between 0 and 1</p>]]></content><author><name>Arize Emmanuel</name></author><category term="machine-learning-python" /><summary type="html"><![CDATA[In this article, we will learn about , what it is, how it works and we will implement it sampling using Python]]></summary></entry></feed>