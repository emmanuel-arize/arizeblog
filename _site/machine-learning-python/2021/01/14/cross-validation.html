<!DOCTYPE html>
<html lang=" en ">
<style>
    #ma {
        margin-right: 150px;
        background-color: white;
    }
    
    body {
        font-size: 20px;
    }
</style><head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Cross-Validation (ML-python) | ARIZE-BLOG</title>
<meta name="generator" content="Jekyll v4.3.1" />
<meta property="og:title" content="Cross-Validation (ML-python)" />
<meta name="author" content="Arize Emmanuel" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="One of the finest techniques to check the generalization power of a machine learning model is to use Cross-validation techniques. Cross-validation refers to a set of methods for measuring the performance of a given predictive model. It can be computationally expensive, because they involve fitting the same model multiple times using different subsets of the training data. Cross-validation techniques generally involves the following process:" />
<meta property="og:description" content="One of the finest techniques to check the generalization power of a machine learning model is to use Cross-validation techniques. Cross-validation refers to a set of methods for measuring the performance of a given predictive model. It can be computationally expensive, because they involve fitting the same model multiple times using different subsets of the training data. Cross-validation techniques generally involves the following process:" />
<link rel="canonical" href="http://localhost:4000/arizeblog/machine-learning-python/2021/01/14/cross-validation.html" />
<meta property="og:url" content="http://localhost:4000/arizeblog/machine-learning-python/2021/01/14/cross-validation.html" />
<meta property="og:site_name" content="ARIZE-BLOG" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-14T03:02:23+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Cross-Validation (ML-python)" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Arize Emmanuel"},"dateModified":"2021-01-14T03:02:23+01:00","datePublished":"2021-01-14T03:02:23+01:00","description":"One of the finest techniques to check the generalization power of a machine learning model is to use Cross-validation techniques. Cross-validation refers to a set of methods for measuring the performance of a given predictive model. It can be computationally expensive, because they involve fitting the same model multiple times using different subsets of the training data. Cross-validation techniques generally involves the following process:","headline":"Cross-Validation (ML-python)","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/arizeblog/machine-learning-python/2021/01/14/cross-validation.html"},"url":"http://localhost:4000/arizeblog/machine-learning-python/2021/01/14/cross-validation.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/arizeblog/assets/main.css">
    <link rel="stylesheet" href="/arizeblog/%20/assets/css/w3.css">
    <link rel="stylesheet" href="/arizeblog/%20/assets/css/bootstrap.min.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/arizeblog/feed.xml" title="ARIZE-BLOG" /></head>
<body class="w3-light-grey"><!-- Sidebar -->
<div class="w3-top">

    <div class="w3-container w3-bar w3-gray ">
        <div style="padding-left: 100px">

            <a href="/arizeblog/" class="w3-bar-item w3-button w3-hide-small w3-text-black"><b>Home</b></a>

            <a href="/arizeblog/machine-learning-python" class="w3-bar-item w3-button w3-hide-small w3-text-black">
                <b>ML. with Python </b>
            </a>

            <a href=" /arizeblog/machine-learning-R" class="w3-bar-item w3-button w3-hide-small w3-text-black">
                <b>ML. with R </b>
            </a>
            <a href="/arizeblog/Power-BI" class="w3-bar-item w3-button w3-hide-small w3-text-black">
                <b>Power Bi (DAX & M) </b>
            </a>
            <a href="/arizeblog/VBA" class="w3-bar-item w3-button w3-hide-small w3-text-black"><b>VBA</b></a>
            <a href="/arizeblog/Excel" class="w3-bar-item w3-button w3-hide-small w3-text-black"><b> Excel</b></a>


            <a href="/arizeblog/about" class="w3-bar-item w3-button w3-hide-small w3-text-black"><b>About</b></a>
            <a href="javascript:void(0)" class="w3-bar-item w3-button w3-right w3-hide-large w3-hide-medium" onclick="myFunction()">&#9776;</a>
        </div>
    </div>

    <div id="demo" class="w3-bar-block w3-gray w3-hide w3-hide-large w3-hide-medium">
        <a href="/arizeblog/machine-learning-python" class="w3-bar-item w3-button w3-text-black">
            <b>Machine Learning Python</b>
        </a>

        <a href="/arizeblog/machine-learning-R" class="w3-bar-item w3-button w3-text-black">
            <b>Machine Learning R</b>
        </a>

        <a href=" /arizeblog/Power-BI" class="w3-bar-item w3-button w3-text-black">
            <b>Power BI (DAX & M)</b>
        </a>

        <a href="/arizeblog/VBA" class="w3-bar-item w3-button w3-text-black w3-text-black">
            <b>VBA</b>
        </a>

        <a href="/arizeblog/Excel" class="w3-bar-item w3-button w3-hide-small w3-text-black"><b>Excel</b> </a>
        <a href="/arizeblogg/about" class="w3-bar-item w3-button w3-text-black"><b>About </b></a>
    </div>
</div>

<script>
    function myFunction() {
        var x = document.getElementById("demo");
        if (x.className.indexOf("w3-show") == -1) {
            x.className += " w3-show";
        } else {
            x.className = x.className.replace(" w3-show", "");
        }
    }
</script>
<br>
    <br style="margin-top:10px;">
    <div id="ma" class="w3-container w3-margin" style="margin-left: 100px;">
        <style media="screen">
    .pad {
        padding-top: 50px;
        padding-right: 80px;
        padding-bottom: 50px;
        padding-left: 80px
    }
</style>

<br style="margin-top:10px;">
<div class="w3-row pad">
    <div style="margin-right:80px;margin-left: 80px; background-color:white;">

        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

            <header class="post-header">
                <h2 class="post-title p-name" itemprop="name headline">Cross-Validation (ML-python)</h2>
                <p class="post-meta">
                    <time class="dt-published" datetime="2021-01-14T03:02:23+01:00" itemprop="datePublished">Jan 14, 2021
                    </time>â€¢ <span itemprop="author" itemscope itemtype="http://schema.org/Person">
                        <span class="p-author h-card" itemprop="name">
                            By Arize Emmanuel
                        </span>
                    </span></p>
            </header>

            <div class="post-content e-content" itemprop="articleBody">
                <span class="w3-container"> <p>One of the finest techniques to check the generalization power of a machine learning model is to use <strong><em>Cross-validation techniques</em></strong>. <strong>Cross-validation</strong> refers to a set of methods for measuring the performance of a given predictive model. It can be computationally expensive, because they involve fitting the same model multiple times using different subsets of the training data. Cross-validation techniques generally involves the following process:</p>

<ol>
  <li>
    <p>Divide the available data set into two sets namely training and testing (validation) data set.</p>
  </li>
  <li>
    <p>Train the model using the training set</p>
  </li>
  <li>
    <p>Test the effectiveness of the model on the reserved sample (testing) of the data set and estimate the prediction error.</p>
  </li>
</ol>

<p><strong>cross-validation methods for assessing model performance includes,</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     Validation set approach (or data split)
     Leave One Out Cross Validation
     k-fold Cross Validation
     Repeated k-fold Cross Validation
</code></pre></div></div>

<h3><b> Validation Set Approach</b></h3>
<p>The validation set approach involves</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> 1.  randomly dividing the available data set into two parts namely,  training data set and validation data set.

 2.  Train the model on the training data set

 3.  The Trained model is then used to predict observations in the validation   set to test the generalization

   ability of  the model when faced with new observations by calculating the prediction error.
</code></pre></div></div>
<p> Let now import the needed packages for the tutorial </p>
<pre><code class="language-{python}">from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split,LeaveOneOut,KFold,RepeatedKFold
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.model_selection import cross_val_score
from sklearn.metrics import make_scorer,mean_squared_error,r2_score
import sklearn
import pandas as pd

</code></pre>
<p>The code below Loads the data to be used</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">marketing</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'data/marking.csv'</span><span class="p">,</span><span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"The advertising datasets has "</span><span class="o">+</span>
      <span class="nb">str</span><span class="p">(</span><span class="n">marketing</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">+</span> <span class="s">' observations and '</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">marketing</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">+</span><span class="s">' features'</span><span class="p">)</span>
</code></pre></div></div>
<p>The advertising datasets has 200 observations and 4 features</p>

<p>displaying the first four instances or observations of the dataset</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">marketing</span><span class="p">.</span><span class="n">head</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
</code></pre></div></div>
<p><img class="w3-center" src="/arizeblog/assets/images/python/crossvalidation/marketing.jpg" /></p>

<h3> <b>Feature Scaling or transformation</b></h3>

<p>Numerical features are often measured on different scales and this variation in scale may pose problems to modelling the data correctly. With few exceptions, variations in scales of numerical features often lead to machine Learning algorithms not performing well. When features have different scales, features with higher magnitude are likely to have higher weights and this affects the performance of the model.</p>

<p><em>Feature scaling is a technique applied as part of the data preparation process in machine learning to put the numerical features on a common scale without distorting the differences in the range of values.</em>
There are many feature scaling techniques such as</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    Min-max scaling (normalization),

    Box-Cox transformation,

    Mean normalization
</code></pre></div></div>

<p>etc, but We will discuss only <b>Standardization(Z-score)</b></p>

<h4><b>Standardization(Z-score) </b></h4>
<p>When Z-score is applied to a feature this makes the feature have a zero mean by subtracting the mean of the feature from the data points and then it divides by the standard deviation so that the resulting distribution has unit variance.</p>

<p><b>Standardization.</b></p>

\[x_{z-score}= \frac{x-\bar{x}}{ \sigma}\]

<p>since all features in our dataset are numerical we will scale them using Z-score</p>

<p><b>Note: the target values is generally not scaled</b></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">scale</span><span class="o">=</span><span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">features</span><span class="o">=</span><span class="n">scale</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">marketing</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="mi">3</span><span class="p">])</span>
<span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>
</code></pre></div></div>
<p><img class="w3-center" src="/arizeblog/assets/images/python/crossvalidation/scale_market.jpg" />
<br /></p>
<p> The code below splits the data into training and testing set with 70% of the instances in the training set and 30% in the testing set</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="o">=</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">marketing</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                                                  <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</code></pre></div></div>
<p> Let now fit a linear regression model to the dataset</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lrg</span><span class="o">=</span><span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lrg</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<p>we now test the trained model on the testing set</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lrg_predictions</span><span class="o">=</span><span class="n">lrg</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div></div>
<p> Let now convert the actual y-values of the testing set and the model predicted values for the testing set into a dataframe and print the first 10 to see how close the predicted y-values  are to that of the actual y-values</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">compare</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">"y-true"</span><span class="p">:</span><span class="n">y_test</span><span class="p">,</span><span class="s">"y-predicted"</span><span class="p">:</span><span class="n">lrg_predictions</span><span class="p">})</span>
<span class="n">compare</span><span class="p">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>
<p><img class="w3-center" src="/arizeblog/assets/images/python/crossvalidation/compare.jpg" /></p>
<p>The code below calculates the mean square error (MSE), root mean square error (RMSE) and the R-Squre of the model based on the test set</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">MSE_lrg</span><span class="o">=</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">lrg_predictions</span><span class="p">,</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">)</span>
<span class="n">RMSE</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">MSE_lrg</span><span class="p">)</span>
<span class="n">R2</span><span class="o">=</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">lrg_predictions</span><span class="p">,</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">)</span>
<span class="n">R2</span><span class="p">,</span><span class="n">MSE_lrg</span><span class="p">,</span><span class="n">RMSE</span>
</code></pre></div></div>
<p>(0.8590575550977458, 3.776792977082013, 1.9433972772138004)</p>

<h4><b> NOTE</b></h4>
<p>the validation set approach is only useful when a large data set is available. The model is trained on only a subset of the data set so it is possible the model will not be able to capture certain patterns or interesting information about data which are only present in the test data, leading to higher bias. The estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.</p>

<h3><b> LEAVE ONE OUT CROSS VALIDATION- LOOCV</b></h3>

<p>LOOCV is a special case of K-cross-validation where the number of folds equals the number of instances in the data set. It involves splitting the date set into two parts. However, instead of creating two subsets of comparable size, only a single data point is reserved as the test set. The model is trained on the training set which consist of all the data points except the reserved point and compute the test error on the reserved data point. It repeats the process until each of the n data points has served as the test set and then average the n test errors.</p>

<p>Let now implement LOOCV</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">Y</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">marketing</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="n">loocv_obj</span><span class="o">=</span><span class="n">LeaveOneOut</span><span class="p">()</span>
<span class="n">error</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">train_idx</span><span class="p">,</span><span class="n">test_idx</span> <span class="ow">in</span> <span class="n">loocv_obj</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>
    <span class="n">X_train</span><span class="o">=</span><span class="n">features</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span>
    <span class="n">y_train</span><span class="o">=</span><span class="n">Y</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span>
    <span class="n">X_test</span><span class="o">=</span><span class="n">features</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
    <span class="n">y_test</span><span class="o">=</span><span class="n">Y</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
    <span class="n">lrg1</span><span class="o">=</span><span class="n">LinearRegression</span><span class="p">()</span>
    <span class="n">lrg1</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    <span class="n">pred</span><span class="o">=</span><span class="n">lrg1</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">MSE</span><span class="o">=</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
    <span class="n">error</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">MSE</span><span class="p">)</span>
<span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">error</span><span class="p">),</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">error</span><span class="p">))</span>

</code></pre></div></div>
<p>(4.2435357128200835, 2.059984396256458)</p>
<p class="w3-center"><b>OR</b></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lrg1</span><span class="o">=</span><span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">loocv</span><span class="o">=</span><span class="n">LeaveOneOut</span><span class="p">()</span>
<span class="n">mse</span><span class="o">=</span><span class="n">make_scorer</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">)</span>
<span class="n">scores</span><span class="o">=</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">lrg1</span><span class="p">,</span><span class="n">features</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">scoring</span><span class="o">=</span><span class="n">mse</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="n">loocv</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Folds: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">scores</span><span class="p">))</span> <span class="o">+</span> <span class="s">", MSE: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">))</span> <span class="o">+</span>
      <span class="s">", RMSE: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">))))</span>
</code></pre></div></div>
<p>Folds: 200, MSE: 4.2435357128200835, RMSE: 2.059984396256458
<br /></p>

<p>Although in LOOCV method, we make use all data points reducing potential bias, it is a poor estimate because it is highly variable, since it is based upon a single observation especially if some data points are outliers and has higher execution time when n is extremely large.</p>

<h3><b> K-Fold Cross-Validation</b></h3>

<p>In practice if we have enough data, we set aside part of the data set known as the validation set and use it to measure the performance of our model prediction but since data are often scarce, this is usually not possible and the best practice in such situations is to use <strong>K-fold cross-validation</strong>.</p>

<h4><b>K-fold cross-validation involves</b></h4>

<ol>
  <li>Randomly splitting the data set into k-subsets (or k-fold)</li>
  <li>Train the model on K-1  subsets</li>
  <li>Test the model on the reserved subset and record the prediction error</li>
  <li>Repeat this process until each of the k subsets has served as the test set.</li>
  <li>The average of the K validation scores is then obtained and used as the validation score for the model and is known as the cross-validation error .</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">cv4</span><span class="o">=</span><span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">lrg1</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">"neg_mean_squared_error"</span><span class="p">,</span>
                        <span class="n">cv</span><span class="o">=</span><span class="n">cv4</span><span class="p">,</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Folds: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">scores</span><span class="p">))</span> <span class="o">+</span> <span class="s">", MSE: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">scores</span><span class="p">)))</span> <span class="o">+</span>
       <span class="s">", RMSE: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">scores</span><span class="p">)))))</span>
</code></pre></div></div>
<p>Folds: 4, MSE: 4.280873800694743, RMSE: 2.0690272595339927</p>

<h3><b>  REPEATED K-FOLD CROSS-VALIDATION</b></h3>

<p>The process of splitting the data into k-folds can be repeated a number of times, this is called repeated k-fold cross validation.</p>

<p>number -the number of folds</p>

<p>repeats	For repeated k-fold cross-validation only: the number of complete sets of folds to compute</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">r2cv4</span> <span class="o">=</span> <span class="n">RepeatedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_repeats</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">lrg1</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">"neg_mean_squared_error"</span><span class="p">,</span>
                          <span class="n">cv</span><span class="o">=</span><span class="n">r2cv4</span> <span class="p">,</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Folds: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">scores</span><span class="p">))</span> <span class="o">+</span> <span class="s">", MSE: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">scores</span><span class="p">)))</span> <span class="o">+</span>
       <span class="s">", RMSE: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">scores</span><span class="p">)))))</span>

</code></pre></div></div>
<p>Folds: 8, MSE: 4.2380837495395145, RMSE: 2.0586606688669007</p>

<h2> References:</h2>
<p>Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. An Introduction to Statistical Learning  with Applications in R.&lt;/a&gt;<br /></p>
</span>
            </div><a class="u-url" href="/arizeblog/machine-learning-python/2021/01/14/cross-validation.html" hidden></a>

        </article>
        <hr>

        <!-- Posts -->
        <br>

        <div class="w3-container w3-light-grey  w3-round-xlarge w3-display-bottom">
            <!-- About Card -->
            <div class=" w3-margin">
                <h4><b>Recommended ML-Python-Posts</b></h4>
            </div>
            <ul class="w3-hoverable ">

                 
                 
                 
                 
                <li class="w3-padding-16">
                    <a href="/arizeblog/machine-learning-python/2021/05/07/INTRODUCTION-TO-LSTM.html">Introduction to LSTMs
                </li>
                
                 
                <li class="w3-padding-16">
                    <a href="/arizeblog/machine-learning-python/2021/05/06/SIMPLE_RNN.html">Introduction to Recurrent Neural Networks (RNNs)
                </li>
                
                 
                <li class="w3-padding-16">
                    <a href="/arizeblog/machine-learning-python/2021/03/22/BOWS.html">Feature Extraction with Bag of Words (BOWs)
                </li>
                
                 
                <li class="w3-padding-16">
                    <a href="/arizeblog/machine-learning-python/2021/02/10/Dropout.html">Dropout Regularization
                </li>
                
                 
                <li class="w3-padding-16">
                    <a href="/arizeblog/machine-learning-python/2021/02/05/ml-django.html">Deploying A Machine Learning Model Using Django
                </li>
                
                 
                <li class="w3-padding-16">
                    <a href="/arizeblog/machine-learning-python/2021/01/24/Logistic-Regression.html">Logistic Regression
                </li>
                
                 
                 
                <li class="w3-padding-16">
                    <a href="/arizeblog/machine-learning-python/2021/01/14/cross-validation.html">Cross-Validation (ML-python)
                </li>
                
                 
                


            </ul>
        </div>

    </div>
    <hr>

</div>
<!--
<div class="w3-card w3-margin">
    <div class="w3-container "><h4>Tags</h4></div>
    <div class="w3-container w3-white">
 
        <span>very good tutorials</span>
         
          <span class="w3-tag w3-light-grey w3-small w3-margin-bottom">
          <a href="/arizeblog/machine-learning-r/2021/01/14/cross-validation.html">Cross-Validation(R) </span>
          


 

  </div>
-->


    </div><footer class="site-footer h-card">
  <data class="u-url" href="/arizeblog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">ARIZE-BLOG</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Emmanuel Arize</li><li><a class="u-email" href="mailto:arize2.emmanuel@gmail.com">arize2.emmanuel@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/emmanuel-arize"><svg class="svg-icon"><use xlink:href="/arizeblog/assets/minima-social-icons.svg#github"></use></svg> <span class="username">emmanuel-arize</span></a></li><li><a href="https://www.linkedin.com/in/emmanuel-arize-277270134"><svg class="svg-icon"><use xlink:href="/arizeblog/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">emmanuel-arize-277270134</span></a></li></ul></div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
<link rel="stylesheet" href="/arizeblog/assets/css/w3.css">
<link rel="stylesheet" href="/arizeblog/assets/css/bootstrap.min.css">
<script id="MathJax-script" async src="/arizeblog/assets/js/tex-mml-chtml.js"></script>
<script id="MathJax-script" async src="/arizeblog/assets/js/w3.js"></script>
<script id="MathJax-script" async src="/arizeblog/assets/js/bootstrap.min.js"></script>

<script type="text/javascript" id="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.0.0/es5/tex-svg.js">
</script>
